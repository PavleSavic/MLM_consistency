{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTyt2CC0siFKeXIDtuiU6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d4cac5105b44e7cba90e07b5d26f3cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aec38db0db324f63a59564963244afc1",
              "IPY_MODEL_10069b9a647845b29772b13c88095e2b",
              "IPY_MODEL_cd513cf32c4946f386ae7e6d3d836810"
            ],
            "layout": "IPY_MODEL_a146a66866a641c6ae5562e3dce40bbc"
          }
        },
        "aec38db0db324f63a59564963244afc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1779155082b543a8827e7faa54453098",
            "placeholder": "​",
            "style": "IPY_MODEL_b1d9e49b37f94f96803cfe3ef34e5e11",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "10069b9a647845b29772b13c88095e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce20d5d029254e1391581739083c089d",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf503bce8c254a03ab85053a5854e135",
            "value": 48
          }
        },
        "cd513cf32c4946f386ae7e6d3d836810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e5d0cf32c7742f3b1664fe4e39f6c15",
            "placeholder": "​",
            "style": "IPY_MODEL_2d37b245d2334789a6273ff37ace5f66",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.93kB/s]"
          }
        },
        "a146a66866a641c6ae5562e3dce40bbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1779155082b543a8827e7faa54453098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1d9e49b37f94f96803cfe3ef34e5e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce20d5d029254e1391581739083c089d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf503bce8c254a03ab85053a5854e135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e5d0cf32c7742f3b1664fe4e39f6c15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d37b245d2334789a6273ff37ace5f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "834ba6ce2a564c8d890d14d44297482f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45605c0d19e241e9bc806987dda31505",
              "IPY_MODEL_27265d4e651242389f1e0504040265ad",
              "IPY_MODEL_b741896e78624da5bf1c65c51f2a22b7"
            ],
            "layout": "IPY_MODEL_1707baac21104e589d3204f603b567ff"
          }
        },
        "45605c0d19e241e9bc806987dda31505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09767970db414d05be1c9cf9f17d9d96",
            "placeholder": "​",
            "style": "IPY_MODEL_d22c2622ed8f47c4841a065c51579088",
            "value": "config.json: 100%"
          }
        },
        "27265d4e651242389f1e0504040265ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90256c31802e478ea1f6cff40ea5aa8c",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a58365ae2c0a461d97a72f18a75843a0",
            "value": 570
          }
        },
        "b741896e78624da5bf1c65c51f2a22b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_602646a623a548c3bbf71db0a8e0e779",
            "placeholder": "​",
            "style": "IPY_MODEL_04ffcf1d1d4347eaa77b73a6cac76e65",
            "value": " 570/570 [00:00&lt;00:00, 22.5kB/s]"
          }
        },
        "1707baac21104e589d3204f603b567ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09767970db414d05be1c9cf9f17d9d96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d22c2622ed8f47c4841a065c51579088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90256c31802e478ea1f6cff40ea5aa8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a58365ae2c0a461d97a72f18a75843a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "602646a623a548c3bbf71db0a8e0e779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04ffcf1d1d4347eaa77b73a6cac76e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ddfb150ff0f401ea8a599227a4afa1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4884f350de374fc98c9bb74a11123bff",
              "IPY_MODEL_9203d2f0e5c048ec8e28a31b71761db7",
              "IPY_MODEL_dda61e4372404c15810f303ffd78c73f"
            ],
            "layout": "IPY_MODEL_7d22a65c8f9a4e79a02e76571f0c2983"
          }
        },
        "4884f350de374fc98c9bb74a11123bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b709e62933a4a9199f1cd2eb246e518",
            "placeholder": "​",
            "style": "IPY_MODEL_399ec2fcd75741168fa371c611e6f9cd",
            "value": "vocab.txt: 100%"
          }
        },
        "9203d2f0e5c048ec8e28a31b71761db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00a8b21f9a954fbe865493716c45530f",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ddd2249bfb024a47bf0aed20ab0beebe",
            "value": 231508
          }
        },
        "dda61e4372404c15810f303ffd78c73f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cfab70651dc47268512afdf3cc9e4fe",
            "placeholder": "​",
            "style": "IPY_MODEL_10ac2a26423644d29b25ce309e7f73bc",
            "value": " 232k/232k [00:00&lt;00:00, 3.88MB/s]"
          }
        },
        "7d22a65c8f9a4e79a02e76571f0c2983": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b709e62933a4a9199f1cd2eb246e518": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "399ec2fcd75741168fa371c611e6f9cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00a8b21f9a954fbe865493716c45530f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddd2249bfb024a47bf0aed20ab0beebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cfab70651dc47268512afdf3cc9e4fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10ac2a26423644d29b25ce309e7f73bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0db5b438fa04f478dd0bc83ba133390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6373eb0d94448a295f24821df09c605",
              "IPY_MODEL_f5f5b11bab5c429296bd256376476337",
              "IPY_MODEL_9a77ec10a1ca4750a3f8056343ca9942"
            ],
            "layout": "IPY_MODEL_a2d94983b98f4dd989f9426b8d676157"
          }
        },
        "b6373eb0d94448a295f24821df09c605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f102df1f101c4cfaba9fcf88d9252db2",
            "placeholder": "​",
            "style": "IPY_MODEL_f50a713db0ce49b8b33ede24c6f0bb68",
            "value": "tokenizer.json: 100%"
          }
        },
        "f5f5b11bab5c429296bd256376476337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4967ca2b8244a9c95d7b39fa8f42491",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58b1703e5a6349a59a9738c79a9618e4",
            "value": 466062
          }
        },
        "9a77ec10a1ca4750a3f8056343ca9942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_765a4e83ed834eee8c3f76db53151c3b",
            "placeholder": "​",
            "style": "IPY_MODEL_ab563e81f29e47f68a376782162fb6a3",
            "value": " 466k/466k [00:00&lt;00:00, 9.31MB/s]"
          }
        },
        "a2d94983b98f4dd989f9426b8d676157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f102df1f101c4cfaba9fcf88d9252db2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f50a713db0ce49b8b33ede24c6f0bb68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4967ca2b8244a9c95d7b39fa8f42491": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58b1703e5a6349a59a9738c79a9618e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "765a4e83ed834eee8c3f76db53151c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab563e81f29e47f68a376782162fb6a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d871b2bcee0946349ce3fb467052ea86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9b666ed96d3462d9ad07c247e4f514f",
              "IPY_MODEL_887adb4bcfa94e16943ce91c519cc4de",
              "IPY_MODEL_e4c8cd026a904d9d9a837972abd2887b"
            ],
            "layout": "IPY_MODEL_d15445690d6d4c1db237eb7bac977d20"
          }
        },
        "d9b666ed96d3462d9ad07c247e4f514f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8a579c423d84d6b9d0d81b75a7864c1",
            "placeholder": "​",
            "style": "IPY_MODEL_6f8fc4f0c8d44a37a8dd84c09a1bece2",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "887adb4bcfa94e16943ce91c519cc4de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea5ee109fd3b437a83ef9767fa0f8b6d",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96f01abb382f4681ab8de7c1369a5a31",
            "value": 440473133
          }
        },
        "e4c8cd026a904d9d9a837972abd2887b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a70626cbfb34afaa467bcb560e17d60",
            "placeholder": "​",
            "style": "IPY_MODEL_cf78d0351f824db5893a059fbad159b1",
            "value": " 440M/440M [00:03&lt;00:00, 136MB/s]"
          }
        },
        "d15445690d6d4c1db237eb7bac977d20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8a579c423d84d6b9d0d81b75a7864c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f8fc4f0c8d44a37a8dd84c09a1bece2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea5ee109fd3b437a83ef9767fa0f8b6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96f01abb382f4681ab8de7c1369a5a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a70626cbfb34afaa467bcb560e17d60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf78d0351f824db5893a059fbad159b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavleSavic/MLM_consistency/blob/main/consistency_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тhis notebook presents the final versions of the functions required for testing model accuracy and consistency, as well as the actual testing of the mentioned measures on some of the relations from the datаset. In the final part of the notebook, a method for fine-tuning the model with the aim of increasing consistency is presented."
      ],
      "metadata": {
        "id": "hQyPGZ2Xc8aH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUJcYHRF4-Hc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "import logging\n",
        "import heapq\n",
        "from typing import Callable\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#!pip install transformers datasets evaluate\n",
        "from transformers import AutoTokenizer, TFAutoModelForMaskedLM, TFAutoModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "TqFXWSWMjkFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66908a55-a135-4b2f-dd51-aef48963db6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proposed functions for making multi-token predictions"
      ],
      "metadata": {
        "id": "7dAHashHkmv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_input_format(input):\n",
        "  new_input = input.replace('[MASK]','<mask>')\n",
        "  return new_input"
      ],
      "metadata": {
        "id": "tgg_B5WalhPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_masks(text:str, num_masks_to_keep: int, mask_str='[MASK]'):\n",
        "    parts = text.split('[MASK]')\n",
        "    num_masks = len(parts) - 1\n",
        "\n",
        "    if num_masks_to_keep > num_masks:\n",
        "        raise ValueError(f\"The text only contains {num_masks} '[MASK]' tokens, but {num_masks_to_keep} were requested to keep!\")\n",
        "\n",
        "    reduced_text = ' [MASK]'.join(part.strip() for part in parts[:num_masks_to_keep + 1])\n",
        "    remaining_text = ''.join(parts[num_masks_to_keep + 1:]).strip()\n",
        "\n",
        "    if remaining_text:\n",
        "        reduced_text += ' ' + remaining_text\n",
        "\n",
        "    return reduced_text.strip()"
      ],
      "metadata": {
        "id": "JhDTSjLLmwKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional MLM\n",
        "# filling masks in parallel independently (Independent approach)\n",
        "# trying different mask token sequence lengths\n",
        "def fill_masks_independently(model_checkpoint: str, inputs: list[str], candidate_set_tokens=None, verbose=0):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint, from_pt=True)\n",
        "\n",
        "    mask_str = '[MASK]'\n",
        "    # Adjusting inputs for RoBERTa models\n",
        "    if 'roberta' in model_checkpoint:\n",
        "        mask_str = '<mask>'\n",
        "        inputs = [change_input_format(input) for input in inputs]\n",
        "\n",
        "    # model_max_length field not set by default for BioBERT and BioMedBERT models\n",
        "    if 'bio' in model_checkpoint.lower():\n",
        "        tokenizer.model_max_length = 512\n",
        "\n",
        "    if verbose:\n",
        "        print(f'Chosen model: {model_checkpoint}')\n",
        "        model.summary()\n",
        "\n",
        "    # if candidate_set_tokens is None, setting it to all tokens of the model\n",
        "    if candidate_set_tokens is None:\n",
        "      candidate_set_tokens = list(tokenizer.get_vocab().values()) # .keys() - decoded tokens (words/subwords)\n",
        "\n",
        "    outputs = []\n",
        "    outputs_decoded = []\n",
        "\n",
        "    for input in inputs:\n",
        "      # trying different mask token sequence lengths - from 1 to M (length of longest true answer)\n",
        "      M = input.count(mask_str)\n",
        "\n",
        "      max_confidence = 0\n",
        "      most_confident_prediction = None\n",
        "\n",
        "      for i in range(1, M+1):\n",
        "\n",
        "        input_text = reduce_masks(input, i, mask_str)\n",
        "        tokenized_input = tokenizer(input_text, return_tensors=\"tf\")\n",
        "\n",
        "        # checking if the model uses token_type_ids (not used in RoBERTa models)\n",
        "        use_token_type_ids = 'token_type_ids' in tokenized_input\n",
        "\n",
        "        # getting the token logits from the model\n",
        "        token_logits = model(**tokenized_input).logits[0]\n",
        "\n",
        "        token_probs = tf.nn.softmax(token_logits, axis=-1)\n",
        "\n",
        "        input_ids = tokenized_input[\"input_ids\"]\n",
        "\n",
        "        mask_token_indices = np.where(input_ids.numpy()[0] == tokenizer.mask_token_id)[0]\n",
        "\n",
        "        mask_token_probs = tf.gather(token_probs, mask_token_indices)\n",
        "\n",
        "        # getting probs of tokens that are present in a candidate set\n",
        "        mask_token_probs_candidates = tf.gather(mask_token_probs, candidate_set_tokens, axis=1)\n",
        "\n",
        "        # tf.matf.top_k returns k top values and indices from the input tensor along last dimension (by default)\n",
        "        top_values, top_indices  = tf.math.top_k(mask_token_probs_candidates, k=1)\n",
        "\n",
        "        # finding original indices (token ids):\n",
        "        # converting candidate_set_tokens to a tf tensor\n",
        "        candidate_set_tokens_tensor = tf.constant(candidate_set_tokens, dtype=tf.int32)\n",
        "        # using tf.gather to transform the indices to corresponding values from candidate_set_tokens_tensor\n",
        "        top_indices_original = tf.gather(candidate_set_tokens_tensor, top_indices)\n",
        "\n",
        "        # confidence - probs of the predicted tokens / number of predicted tokens\n",
        "        confidence = np.sum(tf.squeeze(top_values).numpy()) / len(mask_token_indices)\n",
        "        prediction = list(np.atleast_1d(tf.squeeze(top_indices_original).numpy()))\n",
        "\n",
        "        if verbose:\n",
        "          print(f'Prediction: {prediction} : {tokenizer.convert_ids_to_tokens(prediction)}')\n",
        "          print(f'Confidence: {confidence}')\n",
        "\n",
        "        if confidence > max_confidence:\n",
        "          max_confidence = confidence\n",
        "          most_confident_prediction = prediction\n",
        "\n",
        "      outputs.append(most_confident_prediction)\n",
        "      prediction_decoded = tokenizer.decode(most_confident_prediction, skip_special_tokens=True)\n",
        "      outputs_decoded.append(prediction_decoded)\n",
        "      if verbose:\n",
        "        print('-----------------------------------------------------------------------------------')\n",
        "\n",
        "    return outputs, outputs_decoded"
      ],
      "metadata": {
        "id": "xEfT8zRRjrau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs = [\"Paris is a [MASK] [MASK] to visit.\", \"Jupyter is the largest planet of the [MASK] [MASK].\", \"The weather forecast predicts [MASK] [MASK] for tomorrow.\", \"The weather forecast predicts heavy rain and [MASK] [MASK].\", \"He wanted to visit the museum and explore the [MASK] [MASK].\", \"She was excited about the promotion and [MASK] [MASK].\", \"He is known for his dedication and [MASK] [MASK] [MASK].\", \"They plan to travel to Italy and enjoy the beautiful [MASK] [MASK] [MASK].\",  \"She decided to go to the market and buy some fresh [MASK] [MASK] [MASK] [MASK].\", \"He set a new world record at the [MASK] [MASK] [MASK] [MASK] event.\"]\n",
        "outputs, outputs_dec = fill_masks_independently(bert_models['BERT_base'], test_inputs, verbose=1)\n",
        "\n",
        "test_tokenizer = AutoTokenizer.from_pretrained(bert_models['BERT_base'])\n",
        "\n",
        "i = 0\n",
        "for output, output_dec in zip(outputs, outputs_dec):\n",
        "  print(test_inputs[i])\n",
        "  print(f\"{output} : {output_dec} : {test_tokenizer.convert_ids_to_tokens(output)}\")\n",
        "  i += 1\n",
        "  print('-------------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3d4cac5105b44e7cba90e07b5d26f3cc",
            "aec38db0db324f63a59564963244afc1",
            "10069b9a647845b29772b13c88095e2b",
            "cd513cf32c4946f386ae7e6d3d836810",
            "a146a66866a641c6ae5562e3dce40bbc",
            "1779155082b543a8827e7faa54453098",
            "b1d9e49b37f94f96803cfe3ef34e5e11",
            "ce20d5d029254e1391581739083c089d",
            "bf503bce8c254a03ab85053a5854e135",
            "4e5d0cf32c7742f3b1664fe4e39f6c15",
            "2d37b245d2334789a6273ff37ace5f66",
            "834ba6ce2a564c8d890d14d44297482f",
            "45605c0d19e241e9bc806987dda31505",
            "27265d4e651242389f1e0504040265ad",
            "b741896e78624da5bf1c65c51f2a22b7",
            "1707baac21104e589d3204f603b567ff",
            "09767970db414d05be1c9cf9f17d9d96",
            "d22c2622ed8f47c4841a065c51579088",
            "90256c31802e478ea1f6cff40ea5aa8c",
            "a58365ae2c0a461d97a72f18a75843a0",
            "602646a623a548c3bbf71db0a8e0e779",
            "04ffcf1d1d4347eaa77b73a6cac76e65",
            "0ddfb150ff0f401ea8a599227a4afa1a",
            "4884f350de374fc98c9bb74a11123bff",
            "9203d2f0e5c048ec8e28a31b71761db7",
            "dda61e4372404c15810f303ffd78c73f",
            "7d22a65c8f9a4e79a02e76571f0c2983",
            "5b709e62933a4a9199f1cd2eb246e518",
            "399ec2fcd75741168fa371c611e6f9cd",
            "00a8b21f9a954fbe865493716c45530f",
            "ddd2249bfb024a47bf0aed20ab0beebe",
            "6cfab70651dc47268512afdf3cc9e4fe",
            "10ac2a26423644d29b25ce309e7f73bc",
            "b0db5b438fa04f478dd0bc83ba133390",
            "b6373eb0d94448a295f24821df09c605",
            "f5f5b11bab5c429296bd256376476337",
            "9a77ec10a1ca4750a3f8056343ca9942",
            "a2d94983b98f4dd989f9426b8d676157",
            "f102df1f101c4cfaba9fcf88d9252db2",
            "f50a713db0ce49b8b33ede24c6f0bb68",
            "e4967ca2b8244a9c95d7b39fa8f42491",
            "58b1703e5a6349a59a9738c79a9618e4",
            "765a4e83ed834eee8c3f76db53151c3b",
            "ab563e81f29e47f68a376782162fb6a3",
            "d871b2bcee0946349ce3fb467052ea86",
            "d9b666ed96d3462d9ad07c247e4f514f",
            "887adb4bcfa94e16943ce91c519cc4de",
            "e4c8cd026a904d9d9a837972abd2887b",
            "d15445690d6d4c1db237eb7bac977d20",
            "b8a579c423d84d6b9d0d81b75a7864c1",
            "6f8fc4f0c8d44a37a8dd84c09a1bece2",
            "ea5ee109fd3b437a83ef9767fa0f8b6d",
            "96f01abb382f4681ab8de7c1369a5a31",
            "8a70626cbfb34afaa467bcb560e17d60",
            "cf78d0351f824db5893a059fbad159b1"
          ]
        },
        "id": "tFb51a56vUIU",
        "outputId": "45baa6eb-ebfc-4d52-8bc6-23866b9e828e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d4cac5105b44e7cba90e07b5d26f3cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "834ba6ce2a564c8d890d14d44297482f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ddfb150ff0f401ea8a599227a4afa1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0db5b438fa04f478dd0bc83ba133390"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d871b2bcee0946349ce3fb467052ea86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen model: google-bert/bert-base-uncased\n",
            "Model: \"tf_bert_for_masked_lm\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  108891648 \n",
            "                                                                 \n",
            " mlm___cls (TFBertMLMHead)   multiple                  24459834  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109514298 (417.76 MB)\n",
            "Trainable params: 109514298 (417.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Prediction: [2173] : ['place']\n",
            "Confidence: 0.7230950593948364\n",
            "Prediction: [2307, 2173] : ['great', 'place']\n",
            "Confidence: 0.5290212631225586\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [3103] : ['sun']\n",
            "Confidence: 0.20109564065933228\n",
            "Prediction: [3103, 2155] : ['sun', 'family']\n",
            "Confidence: 0.2359308898448944\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [4633] : ['weather']\n",
            "Confidence: 0.3794039189815521\n",
            "Prediction: [1996, 4633] : ['the', 'weather']\n",
            "Confidence: 0.46575427055358887\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [4586] : ['snow']\n",
            "Confidence: 0.483038067817688\n",
            "Prediction: [4586, 7266] : ['snow', 'winds']\n",
            "Confidence: 0.19853292405605316\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [2103] : ['city']\n",
            "Confidence: 0.13459141552448273\n",
            "Prediction: [2334, 2088] : ['local', 'world']\n",
            "Confidence: 0.07630414515733719\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [4712] : ['promotion']\n",
            "Confidence: 0.11541903018951416\n",
            "Prediction: [1996, 2769] : ['the', 'money']\n",
            "Confidence: 0.15952114760875702\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [12276] : ['dedication']\n",
            "Confidence: 0.18447323143482208\n",
            "Prediction: [3167, 9128] : ['personal', 'determination']\n",
            "Confidence: 0.08673734217882156\n",
            "Prediction: [2010, 3167, 3241] : ['his', 'personal', 'thinking']\n",
            "Confidence: 0.06939616799354553\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [17363] : ['scenery']\n",
            "Confidence: 0.27261894941329956\n",
            "Prediction: [3059, 10833] : ['italian', 'countryside']\n",
            "Confidence: 0.3146092891693115\n",
            "Prediction: [3059, 3059, 10833] : ['italian', 'italian', 'countryside']\n",
            "Confidence: 0.18573009967803955\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [4253] : ['clothes']\n",
            "Confidence: 0.20658248662948608\n",
            "Prediction: [5909, 7852] : ['fruit', 'bread']\n",
            "Confidence: 0.08682030439376831\n",
            "Prediction: [1010, 1998, 7852] : [',', 'and', 'bread']\n",
            "Confidence: 0.1390327513217926\n",
            "Prediction: [11546, 1998, 2000, 2014] : ['vegetables', 'and', 'to', 'her']\n",
            "Confidence: 0.1413540542125702\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [2168] : ['same']\n",
            "Confidence: 0.871084451675415\n",
            "Prediction: [2168, 2168] : ['same', 'same']\n",
            "Confidence: 0.753655195236206\n",
            "Prediction: [2230, 4386, 3783] : ['2010', 'olympic', 'olympics']\n",
            "Confidence: 0.06973844269911449\n",
            "Prediction: [2286, 2088, 2399, 2399] : ['2013', 'world', 'games', 'games']\n",
            "Confidence: 0.10273277014493942\n",
            "-----------------------------------------------------------------------------------\n",
            "Paris is a [MASK] [MASK] to visit.\n",
            "[2173] : place : ['place']\n",
            "-------------------------------------------------------------------------\n",
            "Jupyter is the largest planet of the [MASK] [MASK].\n",
            "[3103, 2155] : sun family : ['sun', 'family']\n",
            "-------------------------------------------------------------------------\n",
            "The weather forecast predicts [MASK] [MASK] for tomorrow.\n",
            "[1996, 4633] : the weather : ['the', 'weather']\n",
            "-------------------------------------------------------------------------\n",
            "The weather forecast predicts heavy rain and [MASK] [MASK].\n",
            "[4586] : snow : ['snow']\n",
            "-------------------------------------------------------------------------\n",
            "He wanted to visit the museum and explore the [MASK] [MASK].\n",
            "[2103] : city : ['city']\n",
            "-------------------------------------------------------------------------\n",
            "She was excited about the promotion and [MASK] [MASK].\n",
            "[1996, 2769] : the money : ['the', 'money']\n",
            "-------------------------------------------------------------------------\n",
            "He is known for his dedication and [MASK] [MASK] [MASK].\n",
            "[12276] : dedication : ['dedication']\n",
            "-------------------------------------------------------------------------\n",
            "They plan to travel to Italy and enjoy the beautiful [MASK] [MASK] [MASK].\n",
            "[3059, 10833] : italian countryside : ['italian', 'countryside']\n",
            "-------------------------------------------------------------------------\n",
            "She decided to go to the market and buy some fresh [MASK] [MASK] [MASK] [MASK].\n",
            "[4253] : clothes : ['clothes']\n",
            "-------------------------------------------------------------------------\n",
            "He set a new world record at the [MASK] [MASK] [MASK] [MASK] event.\n",
            "[2168] : same : ['same']\n",
            "-------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional MLM\n",
        "# filling masks autoregressively (Order approach - left to right)\n",
        "# trying different mask token sequence lengths\n",
        "def fill_masks_autoregressively(model_checkpoint: str, inputs: list[str], candidate_set_tokens=None, verbose=0):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint, from_pt=True)\n",
        "\n",
        "    mask_str = '[MASK]'\n",
        "    # Adjusting inputs for RoBERTa models\n",
        "    if 'roberta' in model_checkpoint:\n",
        "        inputs = [change_input_format(input) for input in inputs]\n",
        "        mask_str = '<mask>'\n",
        "\n",
        "    # model_max_length field not set by default for BioBERT and BioMedBERT models\n",
        "    if 'bio' in model_checkpoint.lower():\n",
        "        tokenizer.model_max_length = 512\n",
        "\n",
        "    if verbose:\n",
        "        print(f'Chosen model: {model_checkpoint}')\n",
        "        model.summary()\n",
        "\n",
        "    # if candidate_set_tokens is None, setting it to all tokens of the model\n",
        "    if candidate_set_tokens is None:\n",
        "      candidate_set_tokens = list(tokenizer.get_vocab().values()) # .keys() - decoded tokens (words/subwords)\n",
        "\n",
        "    outputs = []\n",
        "    outputs_decoded = []\n",
        "\n",
        "    for input in inputs:\n",
        "      # trying different mask token sequence lengths - from 1 to M (length of longest true answer)\n",
        "      M = input.count(mask_str)\n",
        "\n",
        "      max_confidence = 0\n",
        "      most_confident_prediction = None\n",
        "\n",
        "      for i in range(1, M+1):\n",
        "\n",
        "        input_text = reduce_masks(input, i, mask_str)\n",
        "\n",
        "        tokenized_input = tokenizer(input_text, return_tensors=\"tf\")\n",
        "\n",
        "        # checking if the model uses token_type_ids (not used in RoBERTa models)\n",
        "        use_token_type_ids = 'token_type_ids' in tokenized_input\n",
        "\n",
        "        input_ids = tokenized_input[\"input_ids\"]\n",
        "\n",
        "        # finding all positions of the mask tokens\n",
        "        mask_token_indices = np.where(input_ids.numpy()[0] == tokenizer.mask_token_id)[0]\n",
        "\n",
        "        # not necessary to be computed\n",
        "        if verbose:\n",
        "          initial_confidence = 0\n",
        "\n",
        "        prediction = []\n",
        "        for mask_index in mask_token_indices:\n",
        "\n",
        "          token_logits = model(**tokenized_input).logits[0]\n",
        "          token_probs = tf.nn.softmax(token_logits, axis=-1)\n",
        "          mask_token_probs = token_probs[mask_index, :]\n",
        "\n",
        "          # getting the top predicted token from candidate set\n",
        "          top_token = candidate_set_tokens[np.argmax(mask_token_probs.numpy()[candidate_set_tokens])]\n",
        "\n",
        "          if verbose:\n",
        "            initial_confidence += mask_token_probs.numpy()[top_token]\n",
        "\n",
        "          prediction.append(top_token)\n",
        "                                                            # list of tensor coordinates to change\n",
        "          input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, mask_index]], [top_token])\n",
        "\n",
        "          # making new tokenized_input tensor\n",
        "          if use_token_type_ids:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "              'token_type_ids': tokenized_input['token_type_ids']\n",
        "            }\n",
        "          else:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "            }\n",
        "\n",
        "        if verbose:\n",
        "          initial_confidence /= i\n",
        "          print(f'Prediction: {prediction} : {tokenizer.convert_ids_to_tokens(prediction)}')\n",
        "          print(f'Confidence before recomputing: {initial_confidence}')\n",
        "\n",
        "        # recompute confidence of every predicted token (this provides the probability of each token in the context of the entire sequence -  bidirectional conditional distributions)\n",
        "        confidence = 0\n",
        "        for mask_index in mask_token_indices:\n",
        "          predicted_token = tf.gather_nd(input_ids, [[0, mask_index]])[0]\n",
        "\n",
        "          # replacing predicted token with mask to remove bias\n",
        "          input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, mask_index]], [tokenizer.mask_token_id])\n",
        "\n",
        "          if use_token_type_ids:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "              'token_type_ids': tokenized_input['token_type_ids']\n",
        "            }\n",
        "          else:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "            }\n",
        "\n",
        "          token_logits = model(**tokenized_input).logits[0]\n",
        "          token_probs = tf.nn.softmax(token_logits, axis=-1)\n",
        "          mask_token_probs = token_probs[mask_index, :]\n",
        "          # getting prob of predicted token in the context of the entire predicted sequence\n",
        "          confidence += mask_token_probs.numpy()[predicted_token]\n",
        "\n",
        "          # putting predicted token back to the context\n",
        "          input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, mask_index]], [predicted_token])\n",
        "\n",
        "          if use_token_type_ids:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "              'token_type_ids': tokenized_input['token_type_ids']\n",
        "            }\n",
        "          else:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "            }\n",
        "\n",
        "        confidence /= i\n",
        "        if verbose:\n",
        "          print(f'Confidence after recomputing: {confidence}')\n",
        "\n",
        "        if confidence > max_confidence:\n",
        "          max_confidence = confidence\n",
        "          most_confident_prediction = prediction\n",
        "\n",
        "      outputs.append(most_confident_prediction)\n",
        "      prediction_decoded = tokenizer.decode(most_confident_prediction, skip_special_tokens=True)\n",
        "      outputs_decoded.append(prediction_decoded)\n",
        "      if verbose:\n",
        "        print('-----------------------------------------------------------------------------------')\n",
        "\n",
        "    return outputs, outputs_decoded"
      ],
      "metadata": {
        "id": "xyKrDO5PDH6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, outputs_dec = fill_masks_autoregressively(bert_models['BERT_base'], test_inputs, verbose=1)\n",
        "\n",
        "test_tokenizer = AutoTokenizer.from_pretrained(bert_models['BERT_base'])\n",
        "\n",
        "i = 0\n",
        "for output, output_dec in zip(outputs, outputs_dec):\n",
        "  print(test_inputs[i])\n",
        "  print(f\"{output} : {output_dec} : {test_tokenizer.convert_ids_to_tokens(output)}\")\n",
        "  i += 1\n",
        "  print('-------------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVmYJX-lcvxs",
        "outputId": "b0604468-06a3-48be-cc26-67802bc760a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen model: google-bert/bert-base-uncased\n",
            "Model: \"tf_bert_for_masked_lm_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  108891648 \n",
            "                                                                 \n",
            " mlm___cls (TFBertMLMHead)   multiple                  24459834  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109514298 (417.76 MB)\n",
            "Trainable params: 109514298 (417.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Prediction: [2173] : ['place']\n",
            "Confidence before recomputing: 0.7230950593948364\n",
            "Confidence after recomputing: 0.7230950593948364\n",
            "Prediction: [2307, 2173] : ['great', 'place']\n",
            "Confidence before recomputing: 0.5600524544715881\n",
            "Confidence after recomputing: 0.6126892864704132\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [3103] : ['sun']\n",
            "Confidence before recomputing: 0.20109564065933228\n",
            "Confidence after recomputing: 0.20109564065933228\n",
            "Prediction: [3103, 2291] : ['sun', 'system']\n",
            "Confidence before recomputing: 0.3070455491542816\n",
            "Confidence after recomputing: 0.17841963648970705\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [4633] : ['weather']\n",
            "Confidence before recomputing: 0.3794039189815521\n",
            "Confidence after recomputing: 0.3794039189815521\n",
            "Prediction: [1996, 4633] : ['the', 'weather']\n",
            "Confidence before recomputing: 0.5253995656967163\n",
            "Confidence after recomputing: 0.8356338739395142\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [4586] : ['snow']\n",
            "Confidence before recomputing: 0.483038067817688\n",
            "Confidence after recomputing: 0.483038067817688\n",
            "Prediction: [4586, 3785] : ['snow', 'conditions']\n",
            "Confidence before recomputing: 0.18335624039173126\n",
            "Confidence after recomputing: 0.1136405635625124\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [2103] : ['city']\n",
            "Confidence before recomputing: 0.13459141552448273\n",
            "Confidence after recomputing: 0.13459141552448273\n",
            "Prediction: [2334, 2381] : ['local', 'history']\n",
            "Confidence before recomputing: 0.2138642519712448\n",
            "Confidence after recomputing: 0.567966639995575\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [4712] : ['promotion']\n",
            "Confidence before recomputing: 0.11541903018951416\n",
            "Confidence after recomputing: 0.11541903018951416\n",
            "Prediction: [1996, 3105] : ['the', 'job']\n",
            "Confidence before recomputing: 0.18584313616156578\n",
            "Confidence after recomputing: 0.3171945624053478\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [12276] : ['dedication']\n",
            "Confidence before recomputing: 0.18447323143482208\n",
            "Confidence after recomputing: 0.18447323143482208\n",
            "Prediction: [3167, 12276] : ['personal', 'dedication']\n",
            "Confidence before recomputing: 0.0920996367931366\n",
            "Confidence after recomputing: 0.14569056034088135\n",
            "Prediction: [2010, 5541, 2943] : ['his', 'creative', 'energy']\n",
            "Confidence before recomputing: 0.0721778894464175\n",
            "Confidence after recomputing: 0.2102151041229566\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [17363] : ['scenery']\n",
            "Confidence before recomputing: 0.27261894941329956\n",
            "Confidence after recomputing: 0.27261894941329956\n",
            "Prediction: [3059, 10833] : ['italian', 'countryside']\n",
            "Confidence before recomputing: 0.40707463026046753\n",
            "Confidence after recomputing: 0.6007926762104034\n",
            "Prediction: [3059, 2406, 17363] : ['italian', 'country', 'scenery']\n",
            "Confidence before recomputing: 0.14479599396387735\n",
            "Confidence after recomputing: 0.32345331211884815\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [4253] : ['clothes']\n",
            "Confidence before recomputing: 0.20658248662948608\n",
            "Confidence after recomputing: 0.20658248662948608\n",
            "Prediction: [5909, 2612] : ['fruit', 'instead']\n",
            "Confidence before recomputing: 0.2653672359883785\n",
            "Confidence after recomputing: 0.2714965157210827\n",
            "Prediction: [1010, 4840, 4253] : [',', 'fresh', 'clothes']\n",
            "Confidence before recomputing: 0.321824053923289\n",
            "Confidence after recomputing: 0.49307260289788246\n",
            "Prediction: [11546, 2005, 1996, 2154] : ['vegetables', 'for', 'the', 'day']\n",
            "Confidence before recomputing: 0.38971314392983913\n",
            "Confidence after recomputing: 0.6266731545329094\n",
            "-----------------------------------------------------------------------------------\n",
            "Prediction: [2168] : ['same']\n",
            "Confidence before recomputing: 0.871084451675415\n",
            "Confidence after recomputing: 0.871084451675415\n",
            "Prediction: [2168, 2168] : ['same', 'same']\n",
            "Confidence before recomputing: 0.4657422564923763\n",
            "Confidence after recomputing: 0.10493944957852364\n",
            "Prediction: [2230, 4386, 6042] : ['2010', 'olympic', 'qualifying']\n",
            "Confidence before recomputing: 0.15121306975682577\n",
            "Confidence after recomputing: 0.4237557364006837\n",
            "Prediction: [2286, 2621, 3783, 6042] : ['2013', 'summer', 'olympics', 'qualifying']\n",
            "Confidence before recomputing: 0.35553621873259544\n",
            "Confidence after recomputing: 0.34045373071421636\n",
            "-----------------------------------------------------------------------------------\n",
            "Paris is a [MASK] [MASK] to visit.\n",
            "[2173] : place : ['place']\n",
            "-------------------------------------------------------------------------\n",
            "Jupyter is the largest planet of the [MASK] [MASK].\n",
            "[3103] : sun : ['sun']\n",
            "-------------------------------------------------------------------------\n",
            "The weather forecast predicts [MASK] [MASK] for tomorrow.\n",
            "[1996, 4633] : the weather : ['the', 'weather']\n",
            "-------------------------------------------------------------------------\n",
            "The weather forecast predicts heavy rain and [MASK] [MASK].\n",
            "[4586] : snow : ['snow']\n",
            "-------------------------------------------------------------------------\n",
            "He wanted to visit the museum and explore the [MASK] [MASK].\n",
            "[2334, 2381] : local history : ['local', 'history']\n",
            "-------------------------------------------------------------------------\n",
            "She was excited about the promotion and [MASK] [MASK].\n",
            "[1996, 3105] : the job : ['the', 'job']\n",
            "-------------------------------------------------------------------------\n",
            "He is known for his dedication and [MASK] [MASK] [MASK].\n",
            "[2010, 5541, 2943] : his creative energy : ['his', 'creative', 'energy']\n",
            "-------------------------------------------------------------------------\n",
            "They plan to travel to Italy and enjoy the beautiful [MASK] [MASK] [MASK].\n",
            "[3059, 10833] : italian countryside : ['italian', 'countryside']\n",
            "-------------------------------------------------------------------------\n",
            "She decided to go to the market and buy some fresh [MASK] [MASK] [MASK] [MASK].\n",
            "[11546, 2005, 1996, 2154] : vegetables for the day : ['vegetables', 'for', 'the', 'day']\n",
            "-------------------------------------------------------------------------\n",
            "He set a new world record at the [MASK] [MASK] [MASK] [MASK] event.\n",
            "[2168] : same : ['same']\n",
            "-------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional MLM\n",
        "# filling masks sorted by the maximum confidence (Greedy approach)\n",
        "# trying different mask token sequence lengths\n",
        "def fill_masks_by_confidence(model_checkpoint: str, inputs: list[str], candidate_set_tokens=None, verbose=0):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint, from_pt=True)\n",
        "\n",
        "    mask_str = '[MASK]'\n",
        "    # Adjusting inputs for RoBERTa models\n",
        "    if 'roberta' in model_checkpoint:\n",
        "      mask_str = '<mask>'\n",
        "      inputs = [change_input_format(input) for input in inputs]\n",
        "\n",
        "    # model_max_length field not set by default for BioBERT and BioMedBERT models\n",
        "    if 'bio' in model_checkpoint.lower():\n",
        "      tokenizer.model_max_length = 512\n",
        "\n",
        "    if verbose:\n",
        "      print(f'Chosen model: {model_checkpoint}')\n",
        "      model.summary()\n",
        "\n",
        "    # if candidate_set_tokens is None, setting it to all tokens of the model\n",
        "    if candidate_set_tokens is None:\n",
        "      candidate_set_tokens = list(tokenizer.get_vocab().values()) # .keys() - decoded tokens (words/subwords)\n",
        "\n",
        "    outputs = []\n",
        "    outputs_decoded = []\n",
        "\n",
        "    for input in inputs:\n",
        "      # trying different mask token sequence lengths - from 1 to M (length of longest true answer)\n",
        "      M = input.count(mask_str)\n",
        "\n",
        "      max_confidence = 0\n",
        "      most_confident_prediction = None\n",
        "\n",
        "      for i in range(1, M+1):\n",
        "\n",
        "        input_text = reduce_masks(input, i, mask_str)\n",
        "\n",
        "        tokenized_input = tokenizer(input_text, return_tensors=\"tf\")\n",
        "\n",
        "        # checking if the model uses token_type_ids (not used in RoBERTa models)\n",
        "        use_token_type_ids = 'token_type_ids' in tokenized_input\n",
        "\n",
        "        input_ids = tokenized_input[\"input_ids\"]\n",
        "\n",
        "        # needed for confidence recomputation\n",
        "        initial_mask_token_indices = np.where(input_ids.numpy()[0] == tokenizer.mask_token_id)[0]\n",
        "\n",
        "        # not necessary to be computed\n",
        "        if verbose:\n",
        "          initial_confidence = 0\n",
        "\n",
        "        prediction_dict = {}\n",
        "        while True:\n",
        "\n",
        "          # finding all positions of the [MASK] tokens\n",
        "          mask_token_indices = np.where(input_ids.numpy()[0] == tokenizer.mask_token_id)[0]\n",
        "\n",
        "          # all tokens at mask positions are predicted\n",
        "          if len(mask_token_indices) == 0:\n",
        "            break\n",
        "\n",
        "          # getting token logits at mask_token_indices\n",
        "          token_logits = model(**tokenized_input).logits[0]\n",
        "          token_probs = tf.nn.softmax(token_logits, axis=-1)\n",
        "\n",
        "          mask_token_probs = tf.gather(token_probs, mask_token_indices)\n",
        "\n",
        "          mask_token_probs_candidates = tf.gather(mask_token_probs, candidate_set_tokens, axis=1)\n",
        "\n",
        "          # tf.matf.top_k returns k top values and indices from the input tensor along last dimension (by default)\n",
        "          top_values, top_indices = tf.math.top_k(mask_token_probs_candidates, k=1)\n",
        "\n",
        "          # finding original indices (token ids)\n",
        "          # converting candidate_set_tokens to a tf tensor\n",
        "          candidate_set_tokens_tensor = tf.constant(candidate_set_tokens, dtype=tf.int32)\n",
        "          # using tf.gather to transform the indices to corresponding values from candidate_set_tokens_tensor\n",
        "          top_indices_original = tf.gather(candidate_set_tokens_tensor, top_indices)\n",
        "\n",
        "          top_values = np.atleast_1d(tf.squeeze(top_values).numpy())\n",
        "          top_indices_original = np.atleast_1d(tf.squeeze(top_indices_original).numpy())\n",
        "\n",
        "          k = tf.argmax(top_values)\n",
        "          most_confident_mask_position, most_confident_token = mask_token_indices[k], top_indices_original[k]\n",
        "\n",
        "          if verbose:\n",
        "            initial_confidence += top_values[k]\n",
        "\n",
        "          if verbose:\n",
        "            print(f\"{most_confident_token}: {tokenizer.convert_ids_to_tokens([most_confident_token])} - index: {most_confident_mask_position}\")\n",
        "\n",
        "          prediction_dict[most_confident_mask_position] = most_confident_token\n",
        "\n",
        "          input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, most_confident_mask_position]], [most_confident_token])\n",
        "\n",
        "          # making new tokenized_input tensor\n",
        "          if use_token_type_ids:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "              'token_type_ids': tokenized_input['token_type_ids']\n",
        "            }\n",
        "          else:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "            }\n",
        "\n",
        "        prediction = [value for key, value in sorted(prediction_dict.items())]\n",
        "        if verbose:\n",
        "          initial_confidence /= i\n",
        "          print(f'Prediction: {prediction} : {tokenizer.convert_ids_to_tokens(prediction)}')\n",
        "          print(f'Confidence before recomputing: {initial_confidence}')\n",
        "\n",
        "        # recompute confidence of every predicted token (this provides the probability of each token in the context of the entire sequence -  bidirectional conditional distributions)\n",
        "        confidence = 0\n",
        "        for mask_index in initial_mask_token_indices:\n",
        "          predicted_token = prediction_dict[mask_index]\n",
        "\n",
        "          # replacing predicted token with mask to remove bias\n",
        "          input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, mask_index]], [tokenizer.mask_token_id])\n",
        "\n",
        "          if use_token_type_ids:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "              'token_type_ids': tokenized_input['token_type_ids']\n",
        "            }\n",
        "          else:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "            }\n",
        "\n",
        "          token_logits = model(**tokenized_input).logits[0]\n",
        "          token_probs = tf.nn.softmax(token_logits, axis=-1)\n",
        "          mask_token_probs = token_probs[mask_index, :]\n",
        "          # getting prob of predicted token in the context of the entire predicted sequence\n",
        "          confidence += mask_token_probs.numpy()[predicted_token]\n",
        "\n",
        "          # putting predicted token back to the context\n",
        "          input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, mask_index]], [predicted_token])\n",
        "\n",
        "          if use_token_type_ids:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "              'token_type_ids': tokenized_input['token_type_ids']\n",
        "            }\n",
        "          else:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "            }\n",
        "\n",
        "        confidence /= i\n",
        "        if verbose:\n",
        "          print(f'Confidence after recomputing: {confidence}')\n",
        "\n",
        "        if confidence > max_confidence:\n",
        "          max_confidence = confidence\n",
        "          most_confident_prediction = prediction\n",
        "\n",
        "      outputs.append(most_confident_prediction)\n",
        "      prediction_decoded = tokenizer.decode(most_confident_prediction, skip_special_tokens=True)\n",
        "      outputs_decoded.append(prediction_decoded)\n",
        "      if verbose:\n",
        "        print('-----------------------------------------------------------------------------------')\n",
        "\n",
        "    return outputs, outputs_decoded"
      ],
      "metadata": {
        "id": "ns3RFXVJDIwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, outputs_dec = fill_masks_by_confidence(bert_models['BERT_base'], test_inputs, verbose=1)\n",
        "\n",
        "test_tokenizer = AutoTokenizer.from_pretrained(bert_models['BERT_base'])\n",
        "\n",
        "i = 0\n",
        "for output, output_dec in zip(outputs, outputs_dec):\n",
        "  print(test_inputs[i])\n",
        "  print(f\"{output} : {output_dec} : {test_tokenizer.convert_ids_to_tokens(output)}\")\n",
        "  i += 1\n",
        "  print('-------------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxi1ztSL32jz",
        "outputId": "c1c4c07b-3a66-43ab-c98a-d33d62675d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen model: google-bert/bert-base-uncased\n",
            "Model: \"tf_bert_for_masked_lm_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  108891648 \n",
            "                                                                 \n",
            " mlm___cls (TFBertMLMHead)   multiple                  24459834  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109514298 (417.76 MB)\n",
            "Trainable params: 109514298 (417.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "2173: ['place'] - index: 4\n",
            "Prediction: [2173] : ['place']\n",
            "Confidence before recomputing: 0.7230950593948364\n",
            "Confidence after recomputing: 0.7230950593948364\n",
            "2173: ['place'] - index: 5\n",
            "2307: ['great'] - index: 4\n",
            "Prediction: [2307, 2173] : ['great', 'place']\n",
            "Confidence before recomputing: 0.5816580951213837\n",
            "Confidence after recomputing: 0.6126892864704132\n",
            "-----------------------------------------------------------------------------------\n",
            "3103: ['sun'] - index: 10\n",
            "Prediction: [3103] : ['sun']\n",
            "Confidence before recomputing: 0.20109564065933228\n",
            "Confidence after recomputing: 0.20109564065933228\n",
            "3103: ['sun'] - index: 10\n",
            "2291: ['system'] - index: 11\n",
            "Prediction: [3103, 2291] : ['sun', 'system']\n",
            "Confidence before recomputing: 0.3070455491542816\n",
            "Confidence after recomputing: 0.17841963648970705\n",
            "-----------------------------------------------------------------------------------\n",
            "4633: ['weather'] - index: 6\n",
            "Prediction: [4633] : ['weather']\n",
            "Confidence before recomputing: 0.3794039189815521\n",
            "Confidence after recomputing: 0.3794039189815521\n",
            "4633: ['weather'] - index: 7\n",
            "1996: ['the'] - index: 6\n",
            "Prediction: [1996, 4633] : ['the', 'weather']\n",
            "Confidence before recomputing: 0.7759885787963867\n",
            "Confidence after recomputing: 0.8356338739395142\n",
            "-----------------------------------------------------------------------------------\n",
            "4586: ['snow'] - index: 9\n",
            "Prediction: [4586] : ['snow']\n",
            "Confidence before recomputing: 0.483038067817688\n",
            "Confidence after recomputing: 0.483038067817688\n",
            "4586: ['snow'] - index: 9\n",
            "3785: ['conditions'] - index: 10\n",
            "Prediction: [4586, 3785] : ['snow', 'conditions']\n",
            "Confidence before recomputing: 0.18335624039173126\n",
            "Confidence after recomputing: 0.1136405635625124\n",
            "-----------------------------------------------------------------------------------\n",
            "2103: ['city'] - index: 10\n",
            "Prediction: [2103] : ['city']\n",
            "Confidence before recomputing: 0.13459141552448273\n",
            "Confidence after recomputing: 0.13459141552448273\n",
            "2088: ['world'] - index: 11\n",
            "3019: ['natural'] - index: 10\n",
            "Prediction: [3019, 2088] : ['natural', 'world']\n",
            "Confidence before recomputing: 0.26227474957704544\n",
            "Confidence after recomputing: 0.4765520244836807\n",
            "-----------------------------------------------------------------------------------\n",
            "4712: ['promotion'] - index: 8\n",
            "Prediction: [4712] : ['promotion']\n",
            "Confidence before recomputing: 0.11541903018951416\n",
            "Confidence after recomputing: 0.11541903018951416\n",
            "1996: ['the'] - index: 8\n",
            "3105: ['job'] - index: 9\n",
            "Prediction: [1996, 3105] : ['the', 'job']\n",
            "Confidence before recomputing: 0.18584313616156578\n",
            "Confidence after recomputing: 0.3171945624053478\n",
            "-----------------------------------------------------------------------------------\n",
            "12276: ['dedication'] - index: 8\n",
            "Prediction: [12276] : ['dedication']\n",
            "Confidence before recomputing: 0.18447323143482208\n",
            "Confidence after recomputing: 0.18447323143482208\n",
            "9128: ['determination'] - index: 9\n",
            "3167: ['personal'] - index: 8\n",
            "Prediction: [3167, 9128] : ['personal', 'determination']\n",
            "Confidence before recomputing: 0.10848244652152061\n",
            "Confidence after recomputing: 0.04609705810435116\n",
            "2010: ['his'] - index: 8\n",
            "5541: ['creative'] - index: 9\n",
            "2943: ['energy'] - index: 10\n",
            "Prediction: [2010, 5541, 2943] : ['his', 'creative', 'energy']\n",
            "Confidence before recomputing: 0.0721778894464175\n",
            "Confidence after recomputing: 0.2102151041229566\n",
            "-----------------------------------------------------------------------------------\n",
            "17363: ['scenery'] - index: 11\n",
            "Prediction: [17363] : ['scenery']\n",
            "Confidence before recomputing: 0.27261894941329956\n",
            "Confidence after recomputing: 0.27261894941329956\n",
            "10833: ['countryside'] - index: 12\n",
            "3059: ['italian'] - index: 11\n",
            "Prediction: [3059, 10833] : ['italian', 'countryside']\n",
            "Confidence before recomputing: 0.5083273202180862\n",
            "Confidence after recomputing: 0.6007926762104034\n",
            "3059: ['italian'] - index: 12\n",
            "10833: ['countryside'] - index: 13\n",
            "2670: ['southern'] - index: 11\n",
            "Prediction: [2670, 3059, 10833] : ['southern', 'italian', 'countryside']\n",
            "Confidence before recomputing: 0.30884600679079693\n",
            "Confidence after recomputing: 0.4431532919406891\n",
            "-----------------------------------------------------------------------------------\n",
            "4253: ['clothes'] - index: 12\n",
            "Prediction: [4253] : ['clothes']\n",
            "Confidence before recomputing: 0.20658248662948608\n",
            "Confidence after recomputing: 0.20658248662948608\n",
            "7852: ['bread'] - index: 13\n",
            "17776: ['baked'] - index: 12\n",
            "Prediction: [17776, 7852] : ['baked', 'bread']\n",
            "Confidence before recomputing: 0.15325529128313065\n",
            "Confidence after recomputing: 0.4735250771045685\n",
            "1010: [','] - index: 12\n",
            "4840: ['fresh'] - index: 13\n",
            "4253: ['clothes'] - index: 14\n",
            "Prediction: [1010, 4840, 4253] : [',', 'fresh', 'clothes']\n",
            "Confidence before recomputing: 0.321824053923289\n",
            "Confidence after recomputing: 0.49307260289788246\n",
            "1998: ['and'] - index: 13\n",
            "2070: ['some'] - index: 14\n",
            "4253: ['clothes'] - index: 12\n",
            "2833: ['food'] - index: 15\n",
            "Prediction: [4253, 1998, 2070, 2833] : ['clothes', 'and', 'some', 'food']\n",
            "Confidence before recomputing: 0.2503027841448784\n",
            "Confidence after recomputing: 0.6781444251537323\n",
            "-----------------------------------------------------------------------------------\n",
            "2168: ['same'] - index: 9\n",
            "Prediction: [2168] : ['same']\n",
            "Confidence before recomputing: 0.871084451675415\n",
            "Confidence after recomputing: 0.871084451675415\n",
            "2168: ['same'] - index: 9\n",
            "2168: ['same'] - index: 10\n",
            "Prediction: [2168, 2168] : ['same', 'same']\n",
            "Confidence before recomputing: 0.4657422564923763\n",
            "Confidence after recomputing: 0.10493944957852364\n",
            "4386: ['olympic'] - index: 10\n",
            "6042: ['qualifying'] - index: 11\n",
            "2355: ['2016'] - index: 9\n",
            "Prediction: [2355, 4386, 6042] : ['2016', 'olympic', 'qualifying']\n",
            "Confidence before recomputing: 0.15597146997849146\n",
            "Confidence after recomputing: 0.5228117058674494\n",
            "2399: ['games'] - index: 11\n",
            "5663: ['commonwealth'] - index: 10\n",
            "6482: ['athletics'] - index: 12\n",
            "2230: ['2010'] - index: 9\n",
            "Prediction: [2230, 5663, 2399, 6482] : ['2010', 'commonwealth', 'games', 'athletics']\n",
            "Confidence before recomputing: 0.23236512020230293\n",
            "Confidence after recomputing: 0.5744218975305557\n",
            "-----------------------------------------------------------------------------------\n",
            "Paris is a [MASK] [MASK] to visit.\n",
            "[2173] : place : ['place']\n",
            "-------------------------------------------------------------------------\n",
            "Jupyter is the largest planet of the [MASK] [MASK].\n",
            "[3103] : sun : ['sun']\n",
            "-------------------------------------------------------------------------\n",
            "The weather forecast predicts [MASK] [MASK] for tomorrow.\n",
            "[1996, 4633] : the weather : ['the', 'weather']\n",
            "-------------------------------------------------------------------------\n",
            "The weather forecast predicts heavy rain and [MASK] [MASK].\n",
            "[4586] : snow : ['snow']\n",
            "-------------------------------------------------------------------------\n",
            "He wanted to visit the museum and explore the [MASK] [MASK].\n",
            "[3019, 2088] : natural world : ['natural', 'world']\n",
            "-------------------------------------------------------------------------\n",
            "She was excited about the promotion and [MASK] [MASK].\n",
            "[1996, 3105] : the job : ['the', 'job']\n",
            "-------------------------------------------------------------------------\n",
            "He is known for his dedication and [MASK] [MASK] [MASK].\n",
            "[2010, 5541, 2943] : his creative energy : ['his', 'creative', 'energy']\n",
            "-------------------------------------------------------------------------\n",
            "They plan to travel to Italy and enjoy the beautiful [MASK] [MASK] [MASK].\n",
            "[3059, 10833] : italian countryside : ['italian', 'countryside']\n",
            "-------------------------------------------------------------------------\n",
            "She decided to go to the market and buy some fresh [MASK] [MASK] [MASK] [MASK].\n",
            "[4253, 1998, 2070, 2833] : clothes and some food : ['clothes', 'and', 'some', 'food']\n",
            "-------------------------------------------------------------------------\n",
            "He set a new world record at the [MASK] [MASK] [MASK] [MASK] event.\n",
            "[2168] : same : ['same']\n",
            "-------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional MLM\n",
        "# Initial predictions (Order) + Refinement (Order) until predictions converge or maximum number of iterations is reached\n",
        "# trying different mask token sequence lengths\n",
        "# ADD prediction length penalty (???)\n",
        "def fill_masks_autoregressively_with_refinement(model_checkpoint: str, inputs: list[str], candidate_set_tokens=None, max_iter=10, verbose=0):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint, from_pt=True)\n",
        "\n",
        "    mask_str = '[MASK]'\n",
        "    # Adjusting inputs for RoBERTa models\n",
        "    if 'roberta' in model_checkpoint:\n",
        "        mask_str = '<mask>'\n",
        "        inputs = [change_input_format(input) for input in inputs]\n",
        "\n",
        "    # model_max_length field not set by default for BioBERT and BioMedBERT models\n",
        "    if 'bio' in model_checkpoint.lower():\n",
        "        tokenizer.model_max_length = 512\n",
        "\n",
        "    if verbose:\n",
        "        print(f'Chosen model: {model_checkpoint}')\n",
        "        model.summary()\n",
        "\n",
        "    # if candidate_set_tokens is None, setting it to all tokens of the model\n",
        "    if candidate_set_tokens is None:\n",
        "      candidate_set_tokens = list(tokenizer.get_vocab().values()) # .keys() - decoded tokens (words/subwords)\n",
        "\n",
        "    outputs = []\n",
        "    outputs_decoded = []\n",
        "\n",
        "    for input in inputs:\n",
        "      # trying different mask token sequence lengths - from 1 to M (length of longest true answer)\n",
        "      M = input.count(mask_str)\n",
        "\n",
        "      max_confidence = 0\n",
        "      most_confident_prediction = None\n",
        "\n",
        "      for i in range(1, M+1):\n",
        "        input_text = reduce_masks(input, i, mask_str)\n",
        "\n",
        "        tokenized_input = tokenizer(input_text, return_tensors=\"tf\")\n",
        "\n",
        "        # checking if the model uses token_type_ids (not used in RoBERTa models)\n",
        "        use_token_type_ids = 'token_type_ids' in tokenized_input\n",
        "\n",
        "        input_ids = tokenized_input[\"input_ids\"]\n",
        "\n",
        "        # finding all positions of the [MASK] tokens\n",
        "        mask_token_indices = np.where(input_ids.numpy()[0] == tokenizer.mask_token_id)[0]\n",
        "\n",
        "        confidence = 0\n",
        "        prediction_dict = OrderedDict((mask_index, tokenizer.mask_token_id) for mask_index in mask_token_indices)\n",
        "\n",
        "        # making initial predictions\n",
        "        for mask_index in mask_token_indices:\n",
        "\n",
        "          token_logits = model(**tokenized_input).logits[0]\n",
        "          token_probs = tf.nn.softmax(token_logits, axis=-1)\n",
        "          mask_token_probs = token_probs[mask_index, :]\n",
        "\n",
        "          # getting the top predicted token from candidate set\n",
        "          top_token = candidate_set_tokens[np.argmax(mask_token_probs.numpy()[candidate_set_tokens])]\n",
        "          confidence += mask_token_probs.numpy()[top_token]\n",
        "\n",
        "          prediction_dict[mask_index] = top_token\n",
        "\n",
        "          input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, mask_index]], [top_token])\n",
        "\n",
        "          # making new tokenized_input tensor\n",
        "          if use_token_type_ids:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "              'token_type_ids': tokenized_input['token_type_ids']\n",
        "            }\n",
        "          else:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "            }\n",
        "\n",
        "        confidence /= i\n",
        "\n",
        "        if verbose:\n",
        "          prediction_initial = [value for key, value in prediction_dict.items()]\n",
        "          prediction_initial_decoded = tokenizer.decode(prediction_initial, skip_special_tokens=True)\n",
        "          print(f'Initial prediction: {prediction_initial} : {tokenizer.convert_ids_to_tokens(prediction_initial)} : {prediction_initial_decoded}')\n",
        "          print(f'Initial confidence: {confidence}')\n",
        "\n",
        "        # refining predictions - UPDATE: replacing old predicted token with mask token before predicting to remove bias\n",
        "        for j in range(max_iter):\n",
        "\n",
        "          if verbose:\n",
        "            print(f\"Iteration: {j}\")\n",
        "\n",
        "          updated_tokens = 0\n",
        "          new_confidence = 0\n",
        "          for mask_index in mask_token_indices:\n",
        "            predicted_token = prediction_dict[mask_index]\n",
        "\n",
        "            # replacing predicted token with mask token to remove bias\n",
        "            input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, mask_index]], [tokenizer.mask_token_id])\n",
        "\n",
        "            if use_token_type_ids:\n",
        "              tokenized_input = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': tokenized_input['attention_mask'],\n",
        "                'token_type_ids': tokenized_input['token_type_ids']\n",
        "              }\n",
        "            else:\n",
        "              tokenized_input = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': tokenized_input['attention_mask'],\n",
        "              }\n",
        "\n",
        "            token_logits = model(**tokenized_input).logits[0]\n",
        "            token_probs = tf.nn.softmax(token_logits, axis=-1)\n",
        "            mask_token_probs = token_probs[mask_index, :]\n",
        "\n",
        "            # getting the top predicted token from candidate set\n",
        "            top_token = candidate_set_tokens[np.argmax(mask_token_probs.numpy()[candidate_set_tokens])]\n",
        "            new_confidence += mask_token_probs.numpy()[top_token]\n",
        "\n",
        "            if prediction_dict[mask_index] != top_token:\n",
        "              prediction_dict[mask_index] = top_token\n",
        "              updated_tokens += 1\n",
        "\n",
        "            # putting predicted token to the context\n",
        "            input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, mask_index]], [top_token])\n",
        "\n",
        "            # making new tokenized_input tensor\n",
        "            if use_token_type_ids:\n",
        "              tokenized_input = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': tokenized_input['attention_mask'],\n",
        "                'token_type_ids': tokenized_input['token_type_ids']\n",
        "              }\n",
        "            else:\n",
        "              tokenized_input = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': tokenized_input['attention_mask'],\n",
        "              }\n",
        "\n",
        "          # confidence can change even if no tokens are updated (we want the probability of each token recomputed in the context of the entire predicted sequence - bidirectional conditional distributions)\n",
        "          # SMALL ISSUE: if for max_iter iterations prediction changes (no convergence) final prediction confidence won't be recomputed - not happening in tested examples\n",
        "          confidence = new_confidence / i\n",
        "\n",
        "          if verbose:\n",
        "            prediction_j = [value for key, value in prediction_dict.items()]\n",
        "            prediction_j_decoded = tokenizer.decode(prediction_j, skip_special_tokens=True)\n",
        "            print(f'Prediction in iteration {j}: {prediction_j} : {tokenizer.convert_ids_to_tokens(prediction_j)} : {prediction_j_decoded}')\n",
        "            print(f'Confidence in iteration {j}: {confidence}')\n",
        "\n",
        "          # checking if convergence happened\n",
        "          if updated_tokens == 0:\n",
        "            if verbose:\n",
        "              print(f\"\\033[1mConvergence reached in iteration {j}!\\033[0m\")\n",
        "            break\n",
        "\n",
        "        if confidence > max_confidence:\n",
        "          max_confidence = confidence\n",
        "          most_confident_prediction = [value for key, value in prediction_dict.items()]\n",
        "\n",
        "        if verbose:\n",
        "          print('-------------------------------------------------------')\n",
        "\n",
        "      outputs.append(most_confident_prediction)\n",
        "      prediction_decoded = tokenizer.decode(most_confident_prediction, skip_special_tokens=True)\n",
        "      outputs_decoded.append(prediction_decoded)\n",
        "      if verbose:\n",
        "        print('------------------------------------------------------------------------------------------')\n",
        "\n",
        "    return outputs, outputs_decoded"
      ],
      "metadata": {
        "id": "S7_qcDbaDQ90"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, outputs_dec = fill_masks_autoregressively_with_refinement(bert_models['BERT_base'], test_inputs, verbose=1)\n",
        "\n",
        "test_tokenizer = AutoTokenizer.from_pretrained(bert_models['BERT_base'])\n",
        "i = 0\n",
        "for output, output_dec in zip(outputs, outputs_dec):\n",
        "  print(test_inputs[i])\n",
        "  print(f\"{output} : {output_dec} : {test_tokenizer.convert_ids_to_tokens(output)}\")\n",
        "  i += 1\n",
        "  print('-------------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "ZCgHlCth30CV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c37d4130-05a2-4c38-8df4-712f7e5581d0",
        "collapsed": true
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen model: google-bert/bert-base-uncased\n",
            "Model: \"tf_bert_for_masked_lm_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  108891648 \n",
            "                                                                 \n",
            " mlm___cls (TFBertMLMHead)   multiple                  24459834  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109514298 (417.76 MB)\n",
            "Trainable params: 109514298 (417.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Initial prediction: [2173] : ['place'] : place\n",
            "Initial confidence: 0.7230950593948364\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [2173] : ['place'] : place\n",
            "Confidence in iteration 0: 0.7230950593948364\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [2307, 2173] : ['great', 'place'] : great place\n",
            "Initial confidence: 0.5600524544715881\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [2307, 2173] : ['great', 'place'] : great place\n",
            "Confidence in iteration 0: 0.6126892864704132\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [3103] : ['sun'] : sun\n",
            "Initial confidence: 0.20109564065933228\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [3103] : ['sun'] : sun\n",
            "Confidence in iteration 0: 0.20109564065933228\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [3103, 2291] : ['sun', 'system'] : sun system\n",
            "Initial confidence: 0.3070455491542816\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [5943, 2291] : ['solar', 'system'] : solar system\n",
            "Confidence in iteration 0: 0.9963496923446655\n",
            "Iteration: 1\n",
            "Prediction in iteration 1: [5943, 2291] : ['solar', 'system'] : solar system\n",
            "Confidence in iteration 1: 0.9963496923446655\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [4633] : ['weather'] : weather\n",
            "Initial confidence: 0.3794039189815521\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [4633] : ['weather'] : weather\n",
            "Confidence in iteration 0: 0.3794039189815521\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [1996, 4633] : ['the', 'weather'] : the weather\n",
            "Initial confidence: 0.5253995656967163\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [1996, 4633] : ['the', 'weather'] : the weather\n",
            "Confidence in iteration 0: 0.8356338739395142\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [4586] : ['snow'] : snow\n",
            "Initial confidence: 0.483038067817688\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [4586] : ['snow'] : snow\n",
            "Confidence in iteration 0: 0.483038067817688\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [4586, 3785] : ['snow', 'conditions'] : snow conditions\n",
            "Initial confidence: 0.18335624039173126\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [24706, 15717] : ['cloudy', 'skies'] : cloudy skies\n",
            "Confidence in iteration 0: 0.3366182744503021\n",
            "Iteration: 1\n",
            "Prediction in iteration 1: [24706, 15717] : ['cloudy', 'skies'] : cloudy skies\n",
            "Confidence in iteration 1: 0.47586682438850403\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [2103] : ['city'] : city\n",
            "Initial confidence: 0.13459141552448273\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [2103] : ['city'] : city\n",
            "Confidence in iteration 0: 0.13459141552448273\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [2334, 2381] : ['local', 'history'] : local history\n",
            "Initial confidence: 0.2138642519712448\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [2334, 2381] : ['local', 'history'] : local history\n",
            "Confidence in iteration 0: 0.567966639995575\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [4712] : ['promotion'] : promotion\n",
            "Initial confidence: 0.11541903018951416\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [4712] : ['promotion'] : promotion\n",
            "Confidence in iteration 0: 0.11541903018951416\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [1996, 3105] : ['the', 'job'] : the job\n",
            "Initial confidence: 0.18584313616156578\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [1996, 3105] : ['the', 'job'] : the job\n",
            "Confidence in iteration 0: 0.3171945624053478\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [12276] : ['dedication'] : dedication\n",
            "Initial confidence: 0.18447323143482208\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [12276] : ['dedication'] : dedication\n",
            "Confidence in iteration 0: 0.18447323143482208\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [3167, 12276] : ['personal', 'dedication'] : personal dedication\n",
            "Initial confidence: 0.0920996367931366\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [3167, 12276] : ['personal', 'dedication'] : personal dedication\n",
            "Confidence in iteration 0: 0.14569056034088135\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [2010, 5541, 2943] : ['his', 'creative', 'energy'] : his creative energy\n",
            "Initial confidence: 0.0721778894464175\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [2152, 1011, 2943] : ['high', '-', 'energy'] : high - energy\n",
            "Confidence in iteration 0: 0.20485581581791243\n",
            "Iteration: 1\n",
            "Prediction in iteration 1: [2152, 1011, 2943] : ['high', '-', 'energy'] : high - energy\n",
            "Confidence in iteration 1: 0.3604731820523739\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [17363] : ['scenery'] : scenery\n",
            "Initial confidence: 0.27261894941329956\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [17363] : ['scenery'] : scenery\n",
            "Confidence in iteration 0: 0.27261894941329956\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [3059, 10833] : ['italian', 'countryside'] : italian countryside\n",
            "Initial confidence: 0.40707463026046753\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [3059, 10833] : ['italian', 'countryside'] : italian countryside\n",
            "Confidence in iteration 0: 0.6007926762104034\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [3059, 2406, 17363] : ['italian', 'country', 'scenery'] : italian country scenery\n",
            "Initial confidence: 0.14479599396387735\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [3059, 2406, 17363] : ['italian', 'country', 'scenery'] : italian country scenery\n",
            "Confidence in iteration 0: 0.32345331211884815\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [4253] : ['clothes'] : clothes\n",
            "Initial confidence: 0.20658248662948608\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [4253] : ['clothes'] : clothes\n",
            "Confidence in iteration 0: 0.20658248662948608\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [5909, 2612] : ['fruit', 'instead'] : fruit instead\n",
            "Initial confidence: 0.2653672359883785\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [11546, 2612] : ['vegetables', 'instead'] : vegetables instead\n",
            "Confidence in iteration 0: 0.3168068006634712\n",
            "Iteration: 1\n",
            "Prediction in iteration 1: [11546, 2612] : ['vegetables', 'instead'] : vegetables instead\n",
            "Confidence in iteration 1: 0.3168068006634712\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [1010, 4840, 4253] : [',', 'fresh', 'clothes'] : , fresh clothes\n",
            "Initial confidence: 0.321824053923289\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [1010, 4550, 4253] : [',', 'clean', 'clothes'] : , clean clothes\n",
            "Confidence in iteration 0: 0.7304398914178213\n",
            "Iteration: 1\n",
            "Prediction in iteration 1: [1010, 4550, 4253] : [',', 'clean', 'clothes'] : , clean clothes\n",
            "Confidence in iteration 1: 0.6822245419025421\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [11546, 2005, 1996, 2154] : ['vegetables', 'for', 'the', 'day'] : vegetables for the day\n",
            "Initial confidence: 0.38971314392983913\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [11546, 2005, 1996, 2154] : ['vegetables', 'for', 'the', 'day'] : vegetables for the day\n",
            "Confidence in iteration 0: 0.6266731545329094\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [2168] : ['same'] : same\n",
            "Initial confidence: 0.871084451675415\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [2168] : ['same'] : same\n",
            "Confidence in iteration 0: 0.871084451675415\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [2168, 2168] : ['same', 'same'] : same same\n",
            "Initial confidence: 0.4657422564923763\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [2200, 2168] : ['very', 'same'] : very same\n",
            "Confidence in iteration 0: 0.78954216837883\n",
            "Iteration: 1\n",
            "Prediction in iteration 1: [2200, 2168] : ['very', 'same'] : very same\n",
            "Confidence in iteration 1: 0.78954216837883\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [2230, 4386, 6042] : ['2010', 'olympic', 'qualifying'] : 2010 olympic qualifying\n",
            "Initial confidence: 0.15121306975682577\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [2355, 4386, 6042] : ['2016', 'olympic', 'qualifying'] : 2016 olympic qualifying\n",
            "Confidence in iteration 0: 0.5228117058674494\n",
            "Iteration: 1\n",
            "Prediction in iteration 1: [2355, 4386, 6042] : ['2016', 'olympic', 'qualifying'] : 2016 olympic qualifying\n",
            "Confidence in iteration 1: 0.5228117058674494\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [2286, 2621, 3783, 6042] : ['2013', 'summer', 'olympics', 'qualifying'] : 2013 summer olympics qualifying\n",
            "Initial confidence: 0.35553621873259544\n",
            "Iteration: 0\n",
            "Prediction in iteration 0: [2262, 2621, 3783, 6042] : ['2012', 'summer', 'olympics', 'qualifying'] : 2012 summer olympics qualifying\n",
            "Confidence in iteration 0: 0.6170734390616417\n",
            "Iteration: 1\n",
            "Prediction in iteration 1: [2262, 2621, 3783, 6042] : ['2012', 'summer', 'olympics', 'qualifying'] : 2012 summer olympics qualifying\n",
            "Confidence in iteration 1: 0.6170734390616417\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Paris is a [MASK] [MASK] to visit.\n",
            "[2173] : place : ['place']\n",
            "-------------------------------------------------------------------------\n",
            "Jupyter is the largest planet of the [MASK] [MASK].\n",
            "[5943, 2291] : solar system : ['solar', 'system']\n",
            "-------------------------------------------------------------------------\n",
            "The weather forecast predicts [MASK] [MASK] for tomorrow.\n",
            "[1996, 4633] : the weather : ['the', 'weather']\n",
            "-------------------------------------------------------------------------\n",
            "The weather forecast predicts heavy rain and [MASK] [MASK].\n",
            "[4586] : snow : ['snow']\n",
            "-------------------------------------------------------------------------\n",
            "He wanted to visit the museum and explore the [MASK] [MASK].\n",
            "[2334, 2381] : local history : ['local', 'history']\n",
            "-------------------------------------------------------------------------\n",
            "She was excited about the promotion and [MASK] [MASK].\n",
            "[1996, 3105] : the job : ['the', 'job']\n",
            "-------------------------------------------------------------------------\n",
            "He is known for his dedication and [MASK] [MASK] [MASK].\n",
            "[2152, 1011, 2943] : high - energy : ['high', '-', 'energy']\n",
            "-------------------------------------------------------------------------\n",
            "They plan to travel to Italy and enjoy the beautiful [MASK] [MASK] [MASK].\n",
            "[3059, 10833] : italian countryside : ['italian', 'countryside']\n",
            "-------------------------------------------------------------------------\n",
            "She decided to go to the market and buy some fresh [MASK] [MASK] [MASK] [MASK].\n",
            "[1010, 4550, 4253] : , clean clothes : [',', 'clean', 'clothes']\n",
            "-------------------------------------------------------------------------\n",
            "He set a new world record at the [MASK] [MASK] [MASK] [MASK] event.\n",
            "[2168] : same : ['same']\n",
            "-------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional MLM\n",
        "# Initial predictions (Greedy) + Refinement (Order) until predictions converge or maximum number of iterations is reached\n",
        "# trying different mask token sequence lengths\n",
        "# ADD prediction length penalty (???)\n",
        "def fill_masks_by_confidence_order_refinement(model_checkpoint: str, inputs: list[str], candidate_set_tokens=None, top_n=5, max_iter=10, verbose=0):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint, from_pt=True)\n",
        "\n",
        "    mask_str = '[MASK]'\n",
        "    # Adjusting inputs for RoBERTa models\n",
        "    if 'roberta' in model_checkpoint:\n",
        "        mask_str = '<mask>'\n",
        "        inputs = [change_input_format(input) for input in inputs]\n",
        "\n",
        "    # model_max_length field not set by default for BioBERT and BioMedBERT models\n",
        "    if 'bio' in model_checkpoint.lower():\n",
        "        tokenizer.model_max_length = 512\n",
        "\n",
        "    if verbose:\n",
        "        print(f'Chosen model: {model_checkpoint}')\n",
        "        model.summary()\n",
        "\n",
        "    # if candidate_set_tokens is None, setting it to all tokens of the model\n",
        "    if candidate_set_tokens is None:\n",
        "      candidate_set_tokens = list(tokenizer.get_vocab().values()) # .keys() - decoded tokens (words/subwords)\n",
        "\n",
        "    outputs = []\n",
        "    outputs_decoded = []\n",
        "\n",
        "    for input in inputs:\n",
        "      # trying different mask token sequence lengths - from 1 to M (length of longest true answer)\n",
        "      M = input.count(mask_str)\n",
        "\n",
        "      max_confidence = 0\n",
        "      most_confident_prediction = None\n",
        "\n",
        "      for i in range(1, M+1):\n",
        "        input_text = reduce_masks(input, i, mask_str)\n",
        "\n",
        "        tokenized_input = tokenizer(input_text, return_tensors=\"tf\")\n",
        "\n",
        "        # checking if the model uses token_type_ids (not used in RoBERTa models)\n",
        "        use_token_type_ids = 'token_type_ids' in tokenized_input\n",
        "\n",
        "        input_ids = tokenized_input[\"input_ids\"]\n",
        "\n",
        "        # needed for refinement phase\n",
        "        initial_mask_token_indices = np.where(input_ids.numpy()[0] == tokenizer.mask_token_id)[0]\n",
        "\n",
        "        confidence = 0\n",
        "        prediction_dict = {}\n",
        "\n",
        "        # Greedy initial\n",
        "        while True:\n",
        "\n",
        "          # finding all positions of the [MASK] tokens\n",
        "          mask_token_indices = np.where(input_ids.numpy()[0] == tokenizer.mask_token_id)[0]\n",
        "\n",
        "          # all tokens at mask positions are predicted\n",
        "          if len(mask_token_indices) == 0:\n",
        "            break\n",
        "\n",
        "          # getting token logits at mask_token_indices\n",
        "          token_logits = model(**tokenized_input).logits[0]\n",
        "          token_probs = tf.nn.softmax(token_logits, axis=-1)\n",
        "\n",
        "          mask_token_probs = tf.gather(token_probs, mask_token_indices)\n",
        "\n",
        "          mask_token_probs_candidates = tf.gather(mask_token_probs, candidate_set_tokens, axis=1)\n",
        "\n",
        "          # tf.matf.top_k returns k top values and indices from the input tensor along last dimension (by default)\n",
        "          top_values, top_indices = tf.math.top_k(mask_token_probs_candidates, k=1)\n",
        "\n",
        "          # finding original indices (token ids)\n",
        "          # converting candidate_set_tokens to a tf tensor\n",
        "          candidate_set_tokens_tensor = tf.constant(candidate_set_tokens, dtype=tf.int32)\n",
        "          # using tf.gather to transform the indices to corresponding values from candidate_set_tokens_tensor\n",
        "          top_indices_original = tf.gather(candidate_set_tokens_tensor, top_indices)\n",
        "\n",
        "          top_values = np.atleast_1d(tf.squeeze(top_values).numpy())\n",
        "          top_indices_original = np.atleast_1d(tf.squeeze(top_indices_original).numpy())\n",
        "\n",
        "          k = tf.argmax(top_values)\n",
        "          most_confident_mask_position, most_confident_token = mask_token_indices[k], top_indices_original[k]\n",
        "          confidence += top_values[k]\n",
        "\n",
        "          if verbose:\n",
        "            print(f\"{most_confident_token}: {tokenizer.convert_ids_to_tokens([most_confident_token])} - index: {most_confident_mask_position}\")\n",
        "\n",
        "          prediction_dict[most_confident_mask_position] = most_confident_token\n",
        "\n",
        "          input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, most_confident_mask_position]], [most_confident_token])\n",
        "\n",
        "          # making new tokenized_input tensor\n",
        "          if use_token_type_ids:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "              'token_type_ids': tokenized_input['token_type_ids']\n",
        "            }\n",
        "          else:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "            }\n",
        "\n",
        "        confidence /= i\n",
        "        # sorting prediction_dict by mask positions\n",
        "        prediction_dict = OrderedDict({mask_position : prediction_dict[mask_position] for mask_position in sorted(prediction_dict)})\n",
        "\n",
        "        if verbose:\n",
        "          prediction_initial = [value for key, value in prediction_dict.items()]\n",
        "          prediction_initial_decoded = tokenizer.decode(prediction_initial, skip_special_tokens=True)\n",
        "          print(f'Initial greedy prediction: {prediction_initial} : {tokenizer.convert_ids_to_tokens(prediction_initial)} : {prediction_initial_decoded}')\n",
        "          print(f'Initial confidence: {confidence}')\n",
        "\n",
        "        # refining predictions - UPDATE: replacing old predicted token with mask token before predicting to remove bias\n",
        "        for j in range(max_iter):\n",
        "\n",
        "          if verbose:\n",
        "            print(f\"Iteration: {j}\")\n",
        "\n",
        "          updated_tokens = 0\n",
        "          new_confidence = 0\n",
        "          for mask_index in initial_mask_token_indices:\n",
        "            predicted_token = prediction_dict[mask_index]\n",
        "\n",
        "            # replacing predicted token with mask token to remove bias\n",
        "            input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, mask_index]], [tokenizer.mask_token_id])\n",
        "\n",
        "            if use_token_type_ids:\n",
        "              tokenized_input = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': tokenized_input['attention_mask'],\n",
        "                'token_type_ids': tokenized_input['token_type_ids']\n",
        "              }\n",
        "            else:\n",
        "              tokenized_input = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': tokenized_input['attention_mask'],\n",
        "              }\n",
        "\n",
        "            token_logits = model(**tokenized_input).logits[0]\n",
        "            token_probs = tf.nn.softmax(token_logits, axis=-1)\n",
        "            mask_token_probs = token_probs[mask_index, :]\n",
        "\n",
        "            # getting the top predicted token from candidate set\n",
        "            top_token = candidate_set_tokens[np.argmax(mask_token_probs.numpy()[candidate_set_tokens])]\n",
        "            new_confidence += mask_token_probs.numpy()[top_token]\n",
        "\n",
        "            if prediction_dict[mask_index] != top_token:\n",
        "              prediction_dict[mask_index] = top_token\n",
        "              updated_tokens += 1\n",
        "\n",
        "            # putting predicted token to the context\n",
        "            input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, mask_index]], [top_token])\n",
        "\n",
        "            # making new tokenized_input tensor\n",
        "            if use_token_type_ids:\n",
        "              tokenized_input = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': tokenized_input['attention_mask'],\n",
        "                'token_type_ids': tokenized_input['token_type_ids']\n",
        "              }\n",
        "            else:\n",
        "              tokenized_input = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': tokenized_input['attention_mask'],\n",
        "              }\n",
        "\n",
        "          # confidence can change even if no tokens are updated (we want the probability of each token recomputed in the context of the entire predicted sequence - bidirectional conditional distributions)\n",
        "          # SMALL ISSUE: if for max_iter iterations prediction changes (no convergence) final prediction confidence won't be recomputed - not happening in tested examples\n",
        "          confidence = new_confidence / i\n",
        "\n",
        "          if verbose:\n",
        "            prediction_j = [value for key, value in prediction_dict.items()]\n",
        "            prediction_j_decoded = tokenizer.decode(prediction_j, skip_special_tokens=True)\n",
        "            print(f'Prediction in iteration {j} : {prediction_j} : {tokenizer.convert_ids_to_tokens(prediction_j)} : {prediction_j_decoded}')\n",
        "            print(f'Confidence in iteration {j}: {confidence}')\n",
        "\n",
        "          # checking if convergence happened\n",
        "          if updated_tokens == 0:\n",
        "            if verbose:\n",
        "              print(f\"\\033[1mConvergence reached in iteration {j}!\\033[0m\")\n",
        "            break\n",
        "\n",
        "        if confidence > max_confidence:\n",
        "          max_confidence = confidence\n",
        "          most_confident_prediction = [value for key, value in prediction_dict.items()]\n",
        "\n",
        "        if verbose:\n",
        "          print('-------------------------------------------------------')\n",
        "\n",
        "      outputs.append(most_confident_prediction)\n",
        "      prediction_decoded = tokenizer.decode(most_confident_prediction, skip_special_tokens=True)\n",
        "      outputs_decoded.append(prediction_decoded)\n",
        "      if verbose:\n",
        "        print('------------------------------------------------------------------------------------------')\n",
        "\n",
        "    return outputs, outputs_decoded"
      ],
      "metadata": {
        "id": "V3dKOubYDYnk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, outputs_dec = fill_masks_by_confidence_order_refinement(bert_models['BERT_base'], test_inputs, verbose=1)\n",
        "\n",
        "test_tokenizer = AutoTokenizer.from_pretrained(bert_models['BERT_base'])\n",
        "i = 0\n",
        "for output, output_dec in zip(outputs, outputs_dec):\n",
        "  print(test_inputs[i])\n",
        "  print(f\"{output} : {output_dec} : {test_tokenizer.convert_ids_to_tokens(output)}\")\n",
        "  i += 1\n",
        "  print('-------------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H1HUqUg6g-K",
        "outputId": "94b0bf85-38d7-4df0-d31e-a8cea5007dbc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen model: google-bert/bert-base-uncased\n",
            "Model: \"tf_bert_for_masked_lm_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  108891648 \n",
            "                                                                 \n",
            " mlm___cls (TFBertMLMHead)   multiple                  24459834  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109514298 (417.76 MB)\n",
            "Trainable params: 109514298 (417.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "2173: ['place'] - index: 4\n",
            "Initial greedy prediction: [2173] : ['place'] : place\n",
            "Initial confidence: 0.7230950593948364\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [2173] : ['place'] : place\n",
            "Confidence in iteration 0: 0.7230950593948364\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "2173: ['place'] - index: 5\n",
            "2307: ['great'] - index: 4\n",
            "Initial greedy prediction: [2307, 2173] : ['great', 'place'] : great place\n",
            "Initial confidence: 0.5816580951213837\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [2307, 2173] : ['great', 'place'] : great place\n",
            "Confidence in iteration 0: 0.6126892864704132\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "3103: ['sun'] - index: 10\n",
            "Initial greedy prediction: [3103] : ['sun'] : sun\n",
            "Initial confidence: 0.20109564065933228\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [3103] : ['sun'] : sun\n",
            "Confidence in iteration 0: 0.20109564065933228\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "3103: ['sun'] - index: 10\n",
            "2291: ['system'] - index: 11\n",
            "Initial greedy prediction: [3103, 2291] : ['sun', 'system'] : sun system\n",
            "Initial confidence: 0.3070455491542816\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [5943, 2291] : ['solar', 'system'] : solar system\n",
            "Confidence in iteration 0: 0.9963496923446655\n",
            "Iteration: 1\n",
            "Prediction in iteration 1 : [5943, 2291] : ['solar', 'system'] : solar system\n",
            "Confidence in iteration 1: 0.9963496923446655\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "4633: ['weather'] - index: 6\n",
            "Initial greedy prediction: [4633] : ['weather'] : weather\n",
            "Initial confidence: 0.3794039189815521\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [4633] : ['weather'] : weather\n",
            "Confidence in iteration 0: 0.3794039189815521\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "4633: ['weather'] - index: 7\n",
            "1996: ['the'] - index: 6\n",
            "Initial greedy prediction: [1996, 4633] : ['the', 'weather'] : the weather\n",
            "Initial confidence: 0.7759885787963867\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [1996, 4633] : ['the', 'weather'] : the weather\n",
            "Confidence in iteration 0: 0.8356338739395142\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "4586: ['snow'] - index: 9\n",
            "Initial greedy prediction: [4586] : ['snow'] : snow\n",
            "Initial confidence: 0.483038067817688\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [4586] : ['snow'] : snow\n",
            "Confidence in iteration 0: 0.483038067817688\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "4586: ['snow'] - index: 9\n",
            "3785: ['conditions'] - index: 10\n",
            "Initial greedy prediction: [4586, 3785] : ['snow', 'conditions'] : snow conditions\n",
            "Initial confidence: 0.18335624039173126\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [24706, 15717] : ['cloudy', 'skies'] : cloudy skies\n",
            "Confidence in iteration 0: 0.3366182744503021\n",
            "Iteration: 1\n",
            "Prediction in iteration 1 : [24706, 15717] : ['cloudy', 'skies'] : cloudy skies\n",
            "Confidence in iteration 1: 0.47586682438850403\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "2103: ['city'] - index: 10\n",
            "Initial greedy prediction: [2103] : ['city'] : city\n",
            "Initial confidence: 0.13459141552448273\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [2103] : ['city'] : city\n",
            "Confidence in iteration 0: 0.13459141552448273\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "2088: ['world'] - index: 11\n",
            "3019: ['natural'] - index: 10\n",
            "Initial greedy prediction: [3019, 2088] : ['natural', 'world'] : natural world\n",
            "Initial confidence: 0.26227474957704544\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [3019, 2088] : ['natural', 'world'] : natural world\n",
            "Confidence in iteration 0: 0.4765520244836807\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "4712: ['promotion'] - index: 8\n",
            "Initial greedy prediction: [4712] : ['promotion'] : promotion\n",
            "Initial confidence: 0.11541903018951416\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [4712] : ['promotion'] : promotion\n",
            "Confidence in iteration 0: 0.11541903018951416\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "1996: ['the'] - index: 8\n",
            "3105: ['job'] - index: 9\n",
            "Initial greedy prediction: [1996, 3105] : ['the', 'job'] : the job\n",
            "Initial confidence: 0.18584313616156578\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [1996, 3105] : ['the', 'job'] : the job\n",
            "Confidence in iteration 0: 0.3171945624053478\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "12276: ['dedication'] - index: 8\n",
            "Initial greedy prediction: [12276] : ['dedication'] : dedication\n",
            "Initial confidence: 0.18447323143482208\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [12276] : ['dedication'] : dedication\n",
            "Confidence in iteration 0: 0.18447323143482208\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "9128: ['determination'] - index: 9\n",
            "3167: ['personal'] - index: 8\n",
            "Initial greedy prediction: [3167, 9128] : ['personal', 'determination'] : personal determination\n",
            "Initial confidence: 0.10848244652152061\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [3167, 12276] : ['personal', 'dedication'] : personal dedication\n",
            "Confidence in iteration 0: 0.11384474113583565\n",
            "Iteration: 1\n",
            "Prediction in iteration 1 : [3167, 12276] : ['personal', 'dedication'] : personal dedication\n",
            "Confidence in iteration 1: 0.14569056034088135\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "2010: ['his'] - index: 8\n",
            "5541: ['creative'] - index: 9\n",
            "2943: ['energy'] - index: 10\n",
            "Initial greedy prediction: [2010, 5541, 2943] : ['his', 'creative', 'energy'] : his creative energy\n",
            "Initial confidence: 0.0721778894464175\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [2152, 1011, 2943] : ['high', '-', 'energy'] : high - energy\n",
            "Confidence in iteration 0: 0.20485581581791243\n",
            "Iteration: 1\n",
            "Prediction in iteration 1 : [2152, 1011, 2943] : ['high', '-', 'energy'] : high - energy\n",
            "Confidence in iteration 1: 0.3604731820523739\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "17363: ['scenery'] - index: 11\n",
            "Initial greedy prediction: [17363] : ['scenery'] : scenery\n",
            "Initial confidence: 0.27261894941329956\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [17363] : ['scenery'] : scenery\n",
            "Confidence in iteration 0: 0.27261894941329956\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "10833: ['countryside'] - index: 12\n",
            "3059: ['italian'] - index: 11\n",
            "Initial greedy prediction: [3059, 10833] : ['italian', 'countryside'] : italian countryside\n",
            "Initial confidence: 0.5083273202180862\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [3059, 10833] : ['italian', 'countryside'] : italian countryside\n",
            "Confidence in iteration 0: 0.6007926762104034\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "3059: ['italian'] - index: 12\n",
            "10833: ['countryside'] - index: 13\n",
            "2670: ['southern'] - index: 11\n",
            "Initial greedy prediction: [2670, 3059, 10833] : ['southern', 'italian', 'countryside'] : southern italian countryside\n",
            "Initial confidence: 0.30884600679079693\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [2670, 3059, 10833] : ['southern', 'italian', 'countryside'] : southern italian countryside\n",
            "Confidence in iteration 0: 0.4431532919406891\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "4253: ['clothes'] - index: 12\n",
            "Initial greedy prediction: [4253] : ['clothes'] : clothes\n",
            "Initial confidence: 0.20658248662948608\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [4253] : ['clothes'] : clothes\n",
            "Confidence in iteration 0: 0.20658248662948608\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "7852: ['bread'] - index: 13\n",
            "17776: ['baked'] - index: 12\n",
            "Initial greedy prediction: [17776, 7852] : ['baked', 'bread'] : baked bread\n",
            "Initial confidence: 0.15325529128313065\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [17776, 7852] : ['baked', 'bread'] : baked bread\n",
            "Confidence in iteration 0: 0.4735250771045685\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "1010: [','] - index: 12\n",
            "4840: ['fresh'] - index: 13\n",
            "4253: ['clothes'] - index: 14\n",
            "Initial greedy prediction: [1010, 4840, 4253] : [',', 'fresh', 'clothes'] : , fresh clothes\n",
            "Initial confidence: 0.321824053923289\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [1010, 4550, 4253] : [',', 'clean', 'clothes'] : , clean clothes\n",
            "Confidence in iteration 0: 0.7304398914178213\n",
            "Iteration: 1\n",
            "Prediction in iteration 1 : [1010, 4550, 4253] : [',', 'clean', 'clothes'] : , clean clothes\n",
            "Confidence in iteration 1: 0.6822245419025421\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "1998: ['and'] - index: 13\n",
            "2070: ['some'] - index: 14\n",
            "4253: ['clothes'] - index: 12\n",
            "2833: ['food'] - index: 15\n",
            "Initial greedy prediction: [4253, 1998, 2070, 2833] : ['clothes', 'and', 'some', 'food'] : clothes and some food\n",
            "Initial confidence: 0.2503027841448784\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [4253, 1998, 2070, 2833] : ['clothes', 'and', 'some', 'food'] : clothes and some food\n",
            "Confidence in iteration 0: 0.6781444251537323\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "2168: ['same'] - index: 9\n",
            "Initial greedy prediction: [2168] : ['same'] : same\n",
            "Initial confidence: 0.871084451675415\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [2168] : ['same'] : same\n",
            "Confidence in iteration 0: 0.871084451675415\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "2168: ['same'] - index: 9\n",
            "2168: ['same'] - index: 10\n",
            "Initial greedy prediction: [2168, 2168] : ['same', 'same'] : same same\n",
            "Initial confidence: 0.4657422564923763\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [2200, 2168] : ['very', 'same'] : very same\n",
            "Confidence in iteration 0: 0.78954216837883\n",
            "Iteration: 1\n",
            "Prediction in iteration 1 : [2200, 2168] : ['very', 'same'] : very same\n",
            "Confidence in iteration 1: 0.78954216837883\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "4386: ['olympic'] - index: 10\n",
            "6042: ['qualifying'] - index: 11\n",
            "2355: ['2016'] - index: 9\n",
            "Initial greedy prediction: [2355, 4386, 6042] : ['2016', 'olympic', 'qualifying'] : 2016 olympic qualifying\n",
            "Initial confidence: 0.15597146997849146\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [2355, 4386, 6042] : ['2016', 'olympic', 'qualifying'] : 2016 olympic qualifying\n",
            "Confidence in iteration 0: 0.5228117058674494\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "2399: ['games'] - index: 11\n",
            "5663: ['commonwealth'] - index: 10\n",
            "6482: ['athletics'] - index: 12\n",
            "2230: ['2010'] - index: 9\n",
            "Initial greedy prediction: [2230, 5663, 2399, 6482] : ['2010', 'commonwealth', 'games', 'athletics'] : 2010 commonwealth games athletics\n",
            "Initial confidence: 0.23236512020230293\n",
            "Iteration: 0\n",
            "Prediction in iteration 0 : [2230, 5663, 2399, 6482] : ['2010', 'commonwealth', 'games', 'athletics'] : 2010 commonwealth games athletics\n",
            "Confidence in iteration 0: 0.5744218975305557\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Paris is a [MASK] [MASK] to visit.\n",
            "[2173] : place : ['place']\n",
            "-------------------------------------------------------------------------\n",
            "Jupyter is the largest planet of the [MASK] [MASK].\n",
            "[5943, 2291] : solar system : ['solar', 'system']\n",
            "-------------------------------------------------------------------------\n",
            "The weather forecast predicts [MASK] [MASK] for tomorrow.\n",
            "[1996, 4633] : the weather : ['the', 'weather']\n",
            "-------------------------------------------------------------------------\n",
            "The weather forecast predicts heavy rain and [MASK] [MASK].\n",
            "[4586] : snow : ['snow']\n",
            "-------------------------------------------------------------------------\n",
            "He wanted to visit the museum and explore the [MASK] [MASK].\n",
            "[3019, 2088] : natural world : ['natural', 'world']\n",
            "-------------------------------------------------------------------------\n",
            "She was excited about the promotion and [MASK] [MASK].\n",
            "[1996, 3105] : the job : ['the', 'job']\n",
            "-------------------------------------------------------------------------\n",
            "He is known for his dedication and [MASK] [MASK] [MASK].\n",
            "[2152, 1011, 2943] : high - energy : ['high', '-', 'energy']\n",
            "-------------------------------------------------------------------------\n",
            "They plan to travel to Italy and enjoy the beautiful [MASK] [MASK] [MASK].\n",
            "[3059, 10833] : italian countryside : ['italian', 'countryside']\n",
            "-------------------------------------------------------------------------\n",
            "She decided to go to the market and buy some fresh [MASK] [MASK] [MASK] [MASK].\n",
            "[1010, 4550, 4253] : , clean clothes : [',', 'clean', 'clothes']\n",
            "-------------------------------------------------------------------------\n",
            "He set a new world record at the [MASK] [MASK] [MASK] [MASK] event.\n",
            "[2168] : same : ['same']\n",
            "-------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional MLM\n",
        "# Initial predictions (Order) + Refinement (Greedy) until predictions converge or maximum number of iterations is reached\n",
        "# trying different mask token sequence lengths\n",
        "# ADD prediction length penalty (???)\n",
        "def fill_masks_autoregressively_greedy_refinement(model_checkpoint: str, inputs: list[str], candidate_set_tokens=None, max_iter=10, verbose=0):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint, from_pt=True)\n",
        "\n",
        "    mask_str = '[MASK]'\n",
        "    # Adjusting inputs for RoBERTa models\n",
        "    if 'roberta' in model_checkpoint:\n",
        "        mask_str = '<mask>'\n",
        "        inputs = [change_input_format(input) for input in inputs]\n",
        "\n",
        "    # model_max_length field not set by default for BioBERT and BioMedBERT models\n",
        "    if 'bio' in model_checkpoint.lower():\n",
        "        tokenizer.model_max_length = 512\n",
        "\n",
        "    if verbose:\n",
        "        print(f'Chosen model: {model_checkpoint}')\n",
        "        model.summary()\n",
        "\n",
        "    # if candidate_set_tokens is None, setting it to all tokens of the model\n",
        "    if candidate_set_tokens is None:\n",
        "      candidate_set_tokens = list(tokenizer.get_vocab().values()) # .keys() - decoded tokens (words/subwords)\n",
        "\n",
        "    outputs = []\n",
        "    outputs_decoded = []\n",
        "\n",
        "    for input in inputs:\n",
        "      # trying different mask token sequence lengths - from 1 to M (length of longest true answer)\n",
        "      M = input.count(mask_str)\n",
        "\n",
        "      max_confidence = 0\n",
        "      most_confident_prediction = None\n",
        "\n",
        "      for i in range(1, M+1):\n",
        "        input_text = reduce_masks(input, i, mask_str)\n",
        "\n",
        "        tokenized_input = tokenizer(input_text, return_tensors=\"tf\")\n",
        "\n",
        "        # checking if the model uses token_type_ids (not used in RoBERTa models)\n",
        "        use_token_type_ids = 'token_type_ids' in tokenized_input\n",
        "\n",
        "        input_ids = tokenized_input[\"input_ids\"]\n",
        "\n",
        "        # finding all positions of the [MASK] tokens\n",
        "        mask_token_indices = np.where(input_ids.numpy()[0] == tokenizer.mask_token_id)[0]\n",
        "\n",
        "        prediction_dict = OrderedDict((mask_index, tokenizer.mask_token_id) for mask_index in mask_token_indices)\n",
        "\n",
        "        # not necessary to be computed\n",
        "        if verbose:\n",
        "          initial_probs_dict = OrderedDict((mask_index, -1) for mask_index in mask_token_indices)\n",
        "\n",
        "        # making initial predictions\n",
        "        for mask_index in mask_token_indices:\n",
        "\n",
        "          token_logits = model(**tokenized_input).logits[0]\n",
        "          token_probs = tf.nn.softmax(token_logits, axis=-1)\n",
        "          mask_token_probs = token_probs[mask_index, :]\n",
        "\n",
        "          # getting the top predicted token from candidate set\n",
        "          top_token = candidate_set_tokens[np.argmax(mask_token_probs.numpy()[candidate_set_tokens])]\n",
        "          prediction_dict[mask_index] = top_token\n",
        "\n",
        "          if verbose:\n",
        "            token_prob = mask_token_probs.numpy()[top_token]\n",
        "            initial_probs_dict[mask_index] = token_prob\n",
        "\n",
        "          input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, mask_index]], [top_token])\n",
        "\n",
        "          # making new tokenized_input tensor\n",
        "          if use_token_type_ids:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "              'token_type_ids': tokenized_input['token_type_ids']\n",
        "            }\n",
        "          else:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "            }\n",
        "\n",
        "        if verbose:\n",
        "          prediction_initial = [value for key, value in prediction_dict.items()]\n",
        "          initial_probs = [value for key, value in initial_probs_dict.items()]\n",
        "          prediction_initial_decoded = tokenizer.decode(prediction_initial, skip_special_tokens=True)\n",
        "          print(f'Initial prediction: {prediction_initial} : {tokenizer.convert_ids_to_tokens(prediction_initial)} : {prediction_initial_decoded}')\n",
        "          print(f'Initial probs: {initial_probs}')\n",
        "\n",
        "        # refining predictions\n",
        "        for j in range(max_iter):\n",
        "\n",
        "          if verbose:\n",
        "            print(f\"Iteration: {j}\")\n",
        "\n",
        "          # recomputing prob of every token in the context of the entire predicted sequence - bidirectional conditional distributions\n",
        "          probs_dict = OrderedDict((mask_index, -1) for mask_index in mask_token_indices)\n",
        "          for mask_index in mask_token_indices:\n",
        "            predicted_token = prediction_dict[mask_index]\n",
        "\n",
        "            # replacing predicted token with mask to remove bias\n",
        "            input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, mask_index]], [tokenizer.mask_token_id])\n",
        "\n",
        "            if use_token_type_ids:\n",
        "              tokenized_input = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': tokenized_input['attention_mask'],\n",
        "                'token_type_ids': tokenized_input['token_type_ids']\n",
        "              }\n",
        "            else:\n",
        "              tokenized_input = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': tokenized_input['attention_mask'],\n",
        "              }\n",
        "\n",
        "            token_logits = model(**tokenized_input).logits[0]\n",
        "            token_probs = tf.nn.softmax(token_logits, axis=-1)\n",
        "            mask_token_probs = token_probs[mask_index, :]\n",
        "            # getting prob of predicted token in the context of the entire predicted sequence\n",
        "\n",
        "            token_prob = mask_token_probs.numpy()[predicted_token]\n",
        "            probs_dict[mask_index] = token_prob\n",
        "\n",
        "            # putting predicted token back to the context\n",
        "            input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, mask_index]], [predicted_token])\n",
        "\n",
        "            if use_token_type_ids:\n",
        "              tokenized_input = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': tokenized_input['attention_mask'],\n",
        "                'token_type_ids': tokenized_input['token_type_ids']\n",
        "              }\n",
        "            else:\n",
        "              tokenized_input = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': tokenized_input['attention_mask'],\n",
        "              }\n",
        "\n",
        "          if verbose:\n",
        "            probs = [value for key, value in probs_dict.items()]\n",
        "            print(f'Recomputed probs {j}: {probs}')\n",
        "\n",
        "          # finding token with the lowest prob\n",
        "          min_mask_index = min(probs_dict, key=lambda mask_index: probs_dict[mask_index])\n",
        "          min_token = prediction_dict[min_mask_index]\n",
        "\n",
        "          if verbose:\n",
        "            print(f'Token with lowest confidence: {tokenizer.convert_ids_to_tokens(min_token)}')\n",
        "\n",
        "          # repredicting token with lowest confidence\n",
        "          input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, min_mask_index]], [tokenizer.mask_token_id])\n",
        "\n",
        "          if use_token_type_ids:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "              'token_type_ids': tokenized_input['token_type_ids']\n",
        "            }\n",
        "          else:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "            }\n",
        "\n",
        "          token_logits = model(**tokenized_input).logits[0]\n",
        "          token_probs = tf.nn.softmax(token_logits, axis=-1)\n",
        "          min_mask_index_probs = token_probs[min_mask_index, :]\n",
        "\n",
        "          new_predicted_token = candidate_set_tokens[np.argmax(min_mask_index_probs.numpy()[candidate_set_tokens])]\n",
        "\n",
        "          prediction_dict[min_mask_index] = new_predicted_token\n",
        "          probs_dict[min_mask_index] = min_mask_index_probs.numpy()[new_predicted_token]\n",
        "\n",
        "          input_ids = tf.tensor_scatter_nd_update(input_ids, [[0, min_mask_index]], [new_predicted_token])\n",
        "\n",
        "          if use_token_type_ids:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "              'token_type_ids': tokenized_input['token_type_ids']\n",
        "            }\n",
        "          else:\n",
        "            tokenized_input = {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': tokenized_input['attention_mask'],\n",
        "            }\n",
        "\n",
        "          if verbose:\n",
        "            prediction_j = [value for key, value in prediction_dict.items()]\n",
        "            prediction_j_decoded = tokenizer.decode(prediction_j, skip_special_tokens=True)\n",
        "            probs_j = [value for key, value in probs_dict.items()]\n",
        "            print(f'Prediction after iteration {j} : {prediction_j} : {tokenizer.convert_ids_to_tokens(prediction_j)} : {prediction_j_decoded}')\n",
        "            print(f'Probs after iteration {j}: {probs_j}')\n",
        "\n",
        "          # checking if convergence happened\n",
        "          if new_predicted_token == min_token:\n",
        "            if verbose:\n",
        "              print(f\"\\033[1mConvergence reached in iteration {j}!\\033[0m\")\n",
        "            break\n",
        "\n",
        "        confidence = sum([value for key, value in probs_dict.items()]) / i\n",
        "        if confidence > max_confidence:\n",
        "          max_confidence = confidence\n",
        "          most_confident_prediction = [value for key, value in prediction_dict.items()]\n",
        "\n",
        "        if verbose:\n",
        "          print('-------------------------------------------------------')\n",
        "\n",
        "      outputs.append(most_confident_prediction)\n",
        "      prediction_decoded = tokenizer.decode(most_confident_prediction, skip_special_tokens=True)\n",
        "      outputs_decoded.append(prediction_decoded)\n",
        "      if verbose:\n",
        "        print('------------------------------------------------------------------------------------------')\n",
        "\n",
        "    return outputs, outputs_decoded"
      ],
      "metadata": {
        "id": "NFX05dpII3qK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, outputs_dec = fill_masks_autoregressively_greedy_refinement(bert_models['BERT_base'], test_inputs, verbose=1)\n",
        "\n",
        "test_tokenizer = AutoTokenizer.from_pretrained(bert_models['BERT_base'])\n",
        "i = 0\n",
        "for output, output_dec in zip(outputs, outputs_dec):\n",
        "  print(test_inputs[i])\n",
        "  print(f\"{output} : {output_dec} : {test_tokenizer.convert_ids_to_tokens(output)}\")\n",
        "  i += 1\n",
        "  print('-------------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86AJ0XOYaM_W",
        "outputId": "57fea1d1-c30a-49c8-c265-26b05a9fd816"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen model: google-bert/bert-base-uncased\n",
            "Model: \"tf_bert_for_masked_lm_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  108891648 \n",
            "                                                                 \n",
            " mlm___cls (TFBertMLMHead)   multiple                  24459834  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109514298 (417.76 MB)\n",
            "Trainable params: 109514298 (417.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Initial prediction: [2173] : ['place'] : place\n",
            "Initial probs: [0.72309506]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.72309506]\n",
            "Token with lowest confidence: place\n",
            "Prediction after iteration 0 : [2173] : ['place'] : place\n",
            "Probs after iteration 0: [0.72309506]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [2307, 2173] : ['great', 'place'] : great place\n",
            "Initial probs: [0.24371475, 0.87639016]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.3489884, 0.87639016]\n",
            "Token with lowest confidence: great\n",
            "Prediction after iteration 0 : [2307, 2173] : ['great', 'place'] : great place\n",
            "Probs after iteration 0: [0.3489884, 0.87639016]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [3103] : ['sun'] : sun\n",
            "Initial probs: [0.20109564]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.20109564]\n",
            "Token with lowest confidence: sun\n",
            "Prediction after iteration 0 : [3103] : ['sun'] : sun\n",
            "Probs after iteration 0: [0.20109564]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [3103, 2291] : ['sun', 'system'] : sun system\n",
            "Initial probs: [0.25741994, 0.35667115]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.00016811848, 0.35667115]\n",
            "Token with lowest confidence: sun\n",
            "Prediction after iteration 0 : [5943, 2291] : ['solar', 'system'] : solar system\n",
            "Probs after iteration 0: [0.99426407, 0.35667115]\n",
            "Iteration: 1\n",
            "Recomputed probs 1: [0.99426407, 0.9984353]\n",
            "Token with lowest confidence: solar\n",
            "Prediction after iteration 1 : [5943, 2291] : ['solar', 'system'] : solar system\n",
            "Probs after iteration 1: [0.99426407, 0.9984353]\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [4633] : ['weather'] : weather\n",
            "Initial probs: [0.37940392]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.37940392]\n",
            "Token with lowest confidence: weather\n",
            "Prediction after iteration 0 : [4633] : ['weather'] : weather\n",
            "Probs after iteration 0: [0.37940392]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [1996, 4633] : ['the', 'weather'] : the weather\n",
            "Initial probs: [0.34378797, 0.70701116]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.9642566, 0.70701116]\n",
            "Token with lowest confidence: weather\n",
            "Prediction after iteration 0 : [1996, 4633] : ['the', 'weather'] : the weather\n",
            "Probs after iteration 0: [0.9642566, 0.70701116]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [4586] : ['snow'] : snow\n",
            "Initial probs: [0.48303807]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.48303807]\n",
            "Token with lowest confidence: snow\n",
            "Prediction after iteration 0 : [4586] : ['snow'] : snow\n",
            "Probs after iteration 0: [0.48303807]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [4586, 3785] : ['snow', 'conditions'] : snow conditions\n",
            "Initial probs: [0.1989496, 0.16776288]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.05951825, 0.16776288]\n",
            "Token with lowest confidence: snow\n",
            "Prediction after iteration 0 : [24706, 3785] : ['cloudy', 'conditions'] : cloudy conditions\n",
            "Probs after iteration 0: [0.15892208, 0.16776288]\n",
            "Iteration: 1\n",
            "Recomputed probs 1: [0.15892208, 0.20129476]\n",
            "Token with lowest confidence: cloudy\n",
            "Prediction after iteration 1 : [24706, 3785] : ['cloudy', 'conditions'] : cloudy conditions\n",
            "Probs after iteration 1: [0.15892208, 0.20129476]\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [2103] : ['city'] : city\n",
            "Initial probs: [0.13459142]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.13459142]\n",
            "Token with lowest confidence: city\n",
            "Prediction after iteration 0 : [2103] : ['city'] : city\n",
            "Probs after iteration 0: [0.13459142]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [2334, 2381] : ['local', 'history'] : local history\n",
            "Initial probs: [0.04019448, 0.38753402]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.74839926, 0.38753402]\n",
            "Token with lowest confidence: history\n",
            "Prediction after iteration 0 : [2334, 2381] : ['local', 'history'] : local history\n",
            "Probs after iteration 0: [0.74839926, 0.38753402]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [4712] : ['promotion'] : promotion\n",
            "Initial probs: [0.11541903]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.11541903]\n",
            "Token with lowest confidence: promotion\n",
            "Prediction after iteration 0 : [4712] : ['promotion'] : promotion\n",
            "Probs after iteration 0: [0.11541903]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [1996, 3105] : ['the', 'job'] : the job\n",
            "Initial probs: [0.30745187, 0.0642344]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.5701547, 0.0642344]\n",
            "Token with lowest confidence: job\n",
            "Prediction after iteration 0 : [1996, 3105] : ['the', 'job'] : the job\n",
            "Probs after iteration 0: [0.5701547, 0.0642344]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [12276] : ['dedication'] : dedication\n",
            "Initial probs: [0.18447323]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.18447323]\n",
            "Token with lowest confidence: dedication\n",
            "Prediction after iteration 0 : [12276] : ['dedication'] : dedication\n",
            "Probs after iteration 0: [0.18447323]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [3167, 12276] : ['personal', 'dedication'] : personal dedication\n",
            "Initial probs: [0.04207264, 0.14212663]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.14925449, 0.14212663]\n",
            "Token with lowest confidence: dedication\n",
            "Prediction after iteration 0 : [3167, 12276] : ['personal', 'dedication'] : personal dedication\n",
            "Probs after iteration 0: [0.14925449, 0.14212663]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [2010, 5541, 2943] : ['his', 'creative', 'energy'] : his creative energy\n",
            "Initial probs: [0.08862757, 0.06155125, 0.06635485]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.25953183, 0.30475864, 0.06635485]\n",
            "Token with lowest confidence: energy\n",
            "Prediction after iteration 0 : [2010, 5541, 2943] : ['his', 'creative', 'energy'] : his creative energy\n",
            "Probs after iteration 0: [0.25953183, 0.30475864, 0.06635485]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [17363] : ['scenery'] : scenery\n",
            "Initial probs: [0.27261895]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.27261895]\n",
            "Token with lowest confidence: scenery\n",
            "Prediction after iteration 0 : [17363] : ['scenery'] : scenery\n",
            "Probs after iteration 0: [0.27261895]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [3059, 10833] : ['italian', 'countryside'] : italian countryside\n",
            "Initial probs: [0.2944674, 0.5196819]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.6819035, 0.5196819]\n",
            "Token with lowest confidence: countryside\n",
            "Prediction after iteration 0 : [3059, 10833] : ['italian', 'countryside'] : italian countryside\n",
            "Probs after iteration 0: [0.6819035, 0.5196819]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [3059, 2406, 17363] : ['italian', 'country', 'scenery'] : italian country scenery\n",
            "Initial probs: [0.1806121, 0.077788845, 0.17598704]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.6640348, 0.13033812, 0.17598704]\n",
            "Token with lowest confidence: country\n",
            "Prediction after iteration 0 : [3059, 2406, 17363] : ['italian', 'country', 'scenery'] : italian country scenery\n",
            "Probs after iteration 0: [0.6640348, 0.13033812, 0.17598704]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [4253] : ['clothes'] : clothes\n",
            "Initial probs: [0.20658249]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.20658249]\n",
            "Token with lowest confidence: clothes\n",
            "Prediction after iteration 0 : [4253] : ['clothes'] : clothes\n",
            "Probs after iteration 0: [0.20658249]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [5909, 2612] : ['fruit', 'instead'] : fruit instead\n",
            "Initial probs: [0.07742035, 0.45331413]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.089678906, 0.45331413]\n",
            "Token with lowest confidence: fruit\n",
            "Prediction after iteration 0 : [11546, 2612] : ['vegetables', 'instead'] : vegetables instead\n",
            "Probs after iteration 0: [0.20682354, 0.45331413]\n",
            "Iteration: 1\n",
            "Recomputed probs 1: [0.20682354, 0.42679006]\n",
            "Token with lowest confidence: vegetables\n",
            "Prediction after iteration 1 : [11546, 2612] : ['vegetables', 'instead'] : vegetables instead\n",
            "Probs after iteration 1: [0.20682354, 0.42679006]\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [1010, 4840, 4253] : [',', 'fresh', 'clothes'] : , fresh clothes\n",
            "Initial probs: [0.17775607, 0.3296824, 0.45803368]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.97426, 0.046924155, 0.45803368]\n",
            "Token with lowest confidence: fresh\n",
            "Prediction after iteration 0 : [1010, 4550, 4253] : [',', 'clean', 'clothes'] : , clean clothes\n",
            "Probs after iteration 0: [0.97426, 0.49951717, 0.45803368]\n",
            "Iteration: 1\n",
            "Recomputed probs 1: [0.8296139, 0.49951717, 0.7175425]\n",
            "Token with lowest confidence: clean\n",
            "Prediction after iteration 1 : [1010, 4550, 4253] : [',', 'clean', 'clothes'] : , clean clothes\n",
            "Probs after iteration 1: [0.8296139, 0.49951717, 0.7175425]\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [11546, 2005, 1996, 2154] : ['vegetables', 'for', 'the', 'day'] : vegetables for the day\n",
            "Initial probs: [0.08170625, 0.4296236, 0.64441377, 0.40310895]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.19818893, 0.9747308, 0.93066394, 0.40310895]\n",
            "Token with lowest confidence: vegetables\n",
            "Prediction after iteration 0 : [11546, 2005, 1996, 2154] : ['vegetables', 'for', 'the', 'day'] : vegetables for the day\n",
            "Probs after iteration 0: [0.19818893, 0.9747308, 0.93066394, 0.40310895]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Initial prediction: [2168] : ['same'] : same\n",
            "Initial probs: [0.87108445]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.87108445]\n",
            "Token with lowest confidence: same\n",
            "Prediction after iteration 0 : [2168] : ['same'] : same\n",
            "Probs after iteration 0: [0.87108445]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [2168, 2168] : ['same', 'same'] : same same\n",
            "Initial probs: [0.85934234, 0.07214218]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.13773672, 0.07214218]\n",
            "Token with lowest confidence: same\n",
            "Prediction after iteration 0 : [2168, 2168] : ['same', 'same'] : same same\n",
            "Probs after iteration 0: [0.13773672, 0.07214218]\n",
            "\u001b[1mConvergence reached in iteration 0!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [2230, 4386, 6042] : ['2010', 'olympic', 'qualifying'] : 2010 olympic qualifying\n",
            "Initial probs: [0.044758864, 0.10091909, 0.30796126]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.035062734, 0.9282432, 0.30796126]\n",
            "Token with lowest confidence: 2010\n",
            "Prediction after iteration 0 : [2355, 4386, 6042] : ['2016', 'olympic', 'qualifying'] : 2016 olympic qualifying\n",
            "Probs after iteration 0: [0.21645974, 0.9282432, 0.30796126]\n",
            "Iteration: 1\n",
            "Recomputed probs 1: [0.21645974, 0.9539358, 0.39803958]\n",
            "Token with lowest confidence: 2016\n",
            "Prediction after iteration 1 : [2355, 4386, 6042] : ['2016', 'olympic', 'qualifying'] : 2016 olympic qualifying\n",
            "Probs after iteration 1: [0.21645974, 0.9539358, 0.39803958]\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "Initial prediction: [2286, 2621, 3783, 6042] : ['2013', 'summer', 'olympics', 'qualifying'] : 2013 summer olympics qualifying\n",
            "Initial probs: [0.07442033, 0.10485043, 0.97585326, 0.26702085]\n",
            "Iteration: 0\n",
            "Recomputed probs 0: [0.00040493088, 0.21162665, 0.8827625, 0.26702085]\n",
            "Token with lowest confidence: 2013\n",
            "Prediction after iteration 0 : [2262, 2621, 3783, 6042] : ['2012', 'summer', 'olympics', 'qualifying'] : 2012 summer olympics qualifying\n",
            "Probs after iteration 0: [0.296483, 0.21162665, 0.8827625, 0.26702085]\n",
            "Iteration: 1\n",
            "Recomputed probs 1: [0.296483, 0.94447064, 0.92132324, 0.30601686]\n",
            "Token with lowest confidence: 2012\n",
            "Prediction after iteration 1 : [2262, 2621, 3783, 6042] : ['2012', 'summer', 'olympics', 'qualifying'] : 2012 summer olympics qualifying\n",
            "Probs after iteration 1: [0.296483, 0.94447064, 0.92132324, 0.30601686]\n",
            "\u001b[1mConvergence reached in iteration 1!\u001b[0m\n",
            "-------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Paris is a [MASK] [MASK] to visit.\n",
            "[2173] : place : ['place']\n",
            "-------------------------------------------------------------------------\n",
            "Jupyter is the largest planet of the [MASK] [MASK].\n",
            "[5943, 2291] : solar system : ['solar', 'system']\n",
            "-------------------------------------------------------------------------\n",
            "The weather forecast predicts [MASK] [MASK] for tomorrow.\n",
            "[1996, 4633] : the weather : ['the', 'weather']\n",
            "-------------------------------------------------------------------------\n",
            "The weather forecast predicts heavy rain and [MASK] [MASK].\n",
            "[4586] : snow : ['snow']\n",
            "-------------------------------------------------------------------------\n",
            "He wanted to visit the museum and explore the [MASK] [MASK].\n",
            "[2334, 2381] : local history : ['local', 'history']\n",
            "-------------------------------------------------------------------------\n",
            "She was excited about the promotion and [MASK] [MASK].\n",
            "[1996, 3105] : the job : ['the', 'job']\n",
            "-------------------------------------------------------------------------\n",
            "He is known for his dedication and [MASK] [MASK] [MASK].\n",
            "[2010, 5541, 2943] : his creative energy : ['his', 'creative', 'energy']\n",
            "-------------------------------------------------------------------------\n",
            "They plan to travel to Italy and enjoy the beautiful [MASK] [MASK] [MASK].\n",
            "[3059, 10833] : italian countryside : ['italian', 'countryside']\n",
            "-------------------------------------------------------------------------\n",
            "She decided to go to the market and buy some fresh [MASK] [MASK] [MASK] [MASK].\n",
            "[1010, 4550, 4253] : , clean clothes : [',', 'clean', 'clothes']\n",
            "-------------------------------------------------------------------------\n",
            "He set a new world record at the [MASK] [MASK] [MASK] [MASK] event.\n",
            "[2168] : same : ['same']\n",
            "-------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# restricted candidate_set_tokens added as parameter\n",
        "def fill_masks_beam_search(model_checkpoint: str, inputs: list[str], candidate_set_tokens=None, top_n=5, beam_width=5, verbose=0):\n",
        "\n",
        "    if top_n > beam_width:\n",
        "       raise ValueError(\"top_n must be less than or equal to beam_width\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint, from_pt=True)\n",
        "\n",
        "    # Adjusting inputs for RoBERTa models\n",
        "    if 'roberta' in model_checkpoint:\n",
        "        inputs = [change_input_format(input) for input in inputs]\n",
        "\n",
        "    # model_max_length field not set by default for BioBERT and BioMedBERT models\n",
        "    if 'bio' in model_checkpoint.lower():\n",
        "        tokenizer.model_max_length = 512\n",
        "\n",
        "    if verbose:\n",
        "        print(f'Chosen model: {model_checkpoint}')\n",
        "        model.summary()\n",
        "\n",
        "    # if candidate_set_tokens is None, setting it to all tokens of the model\n",
        "    if candidate_set_tokens is None:\n",
        "      candidate_set_tokens = list(tokenizer.get_vocab().values()) # .keys() - decoded tokens (words/subwords)\n",
        "\n",
        "    all_outputs = []\n",
        "    all_outputs_decoded = []\n",
        "    for input in inputs:\n",
        "\n",
        "      tokenized_input = tokenizer(input, return_tensors=\"tf\")\n",
        "\n",
        "      # checking if the model uses token_type_ids (not used in RoBERTa models)\n",
        "      use_token_type_ids = 'token_type_ids' in tokenized_input\n",
        "\n",
        "      input_ids = tokenized_input[\"input_ids\"]\n",
        "\n",
        "      mask_token_id = tokenizer.mask_token_id\n",
        "      mask_token_indices = np.where(input_ids.numpy()[0] == mask_token_id)[0]\n",
        "\n",
        "      # Initializing the beam with the initial tokenized text (input_ids, attention_mask and token_type_ids (optional)) and a score of 0\n",
        "      beam = [(tokenized_input, 0.0)]\n",
        "\n",
        "      # Continue until all masks are filled\n",
        "      for mask_token_index in mask_token_indices:\n",
        "        candidates = []\n",
        "\n",
        "        for seq, score in beam:\n",
        "            token_logits = model(**seq).logits[0]\n",
        "            mask_token_logits = token_logits[mask_token_index, :]\n",
        "\n",
        "            # getting logits of tokens that are present in a candidate set\n",
        "            mask_token_logits_candidates = tf.gather(mask_token_logits, candidate_set_tokens)\n",
        "\n",
        "            # tf.matf.top_k returns k top values and indices from the input tensor along last dimension (by default)\n",
        "            top_k_values, top_k_indices  = tf.math.top_k(mask_token_logits_candidates, k=beam_width)\n",
        "\n",
        "            # finding original indices (token ids):\n",
        "            # converting candidate_set_tokens to a tf tensor\n",
        "            candidate_set_tokens_tensor = tf.constant(candidate_set_tokens, dtype=tf.int32)\n",
        "\n",
        "            # using tf.gather to transform the indices to corresponding values from candidate_set_tokens_tensor\n",
        "            top_k_indices_original = tf.gather(candidate_set_tokens_tensor, top_k_indices)\n",
        "\n",
        "            for token_idx, token_logit in zip(top_k_indices_original, top_k_values):\n",
        "\n",
        "                # creating a new sequence with the predicted token\n",
        "                new_input_ids = tf.tensor_scatter_nd_update(seq['input_ids'], [[0, mask_token_index]], [token_idx])\n",
        "\n",
        "                if use_token_type_ids:\n",
        "                  new_seq = {\n",
        "                    'input_ids': new_input_ids,\n",
        "                    'attention_mask': seq['attention_mask'],\n",
        "                    'token_type_ids': seq['token_type_ids']\n",
        "                  }\n",
        "                else:\n",
        "                  new_seq = {\n",
        "                    'input_ids': new_input_ids,\n",
        "                    'attention_mask': seq['attention_mask'],\n",
        "                  }\n",
        "\n",
        "                # calculating the new score using logits\n",
        "                new_score = score + token_logit.numpy()\n",
        "\n",
        "                # adding the new sequence and its score to the candidates list\n",
        "                candidates.append((new_seq, new_score))\n",
        "\n",
        "          # selecting the top beam_width candidates\n",
        "        beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[1])\n",
        "\n",
        "      # extracting predicted tokens from the top_n sequences\n",
        "      outputs = []\n",
        "      outputs_decoded = []\n",
        "      for seq, _ in heapq.nlargest(top_n, beam, key=lambda x: x[1]):\n",
        "        # prediction: from predicted token at the first to the predicted token at the last mask index (all mask tokens in a sequence)\n",
        "        prediction = seq['input_ids'].numpy()[0][mask_token_indices[0]:mask_token_indices[-1]+1]\n",
        "        prediction_decoded = tokenizer.decode(prediction, skip_special_tokens=True)\n",
        "        outputs.append(list(prediction))\n",
        "        outputs_decoded.append(prediction_decoded)\n",
        "\n",
        "      all_outputs.append(outputs)\n",
        "      all_outputs_decoded.append(outputs_decoded)\n",
        "\n",
        "    return all_outputs, all_outputs_decoded"
      ],
      "metadata": {
        "id": "q-sbGhy3DlZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uncased\n",
        "bert_models = {'BERT_base' : \"google-bert/bert-base-uncased\", 'BERT_large': \"google-bert/bert-large-uncased\",\n",
        "                'BERT_large_wwm': \"google-bert/bert-large-uncased-whole-word-masking\"}\n",
        "# cased\n",
        "roberta_models = {'RoBERTa_base': \"FacebookAI/roberta-base\", 'RoBERTa_large': \"FacebookAI/roberta-large\"}\n",
        "# uncased\n",
        "albert_models = {'ALBERT_base': \"albert/albert-base-v2\", 'ALBERT_xxlarge': \"albert/albert-xxlarge-v2\"}\n",
        "# cased\n",
        "biobert_models = {'BioBERT': \"dmis-lab/biobert-base-cased-v1.2\"}\n",
        "# uncased\n",
        "biomedbert_models = {'BioMedBERT_base_abstract' : \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\",\n",
        "                     'BioMedBERT_base_full': \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\",\n",
        "                     'BioMedBERT_large_abstract': \"microsoft/BiomedNLP-BiomedBERT-large-uncased-abstract\"}"
      ],
      "metadata": {
        "id": "cu86AmMdvPHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DPmI6_KCc7c_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9H_BLL-sc6yj"
      }
    }
  ]
}